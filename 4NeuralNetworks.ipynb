{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supervised learning (briefly)\n",
    "* Feed forward neural nets\n",
    "* Training (backprop)\n",
    "* Convolutional neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Supervised learning\n",
    "\n",
    "In the Supervised Learning setting we are given a dataset $D = \\{(x_1,y_1),...,(x_n,y_n)\\}$ to learn from. The way we \"learn\" is by using D to search a hypothesis space H for some \"best\" hypothesis $g\\in H$. By \"best\", we mean that we want g to be as close to the __ unknown target function f__ as possible, i.e.: \n",
    "\n",
    "$$g \\approx f$$\n",
    "\n",
    "We most often find g by minimizing an in-sample error function $E_{in}(h)$. The process of finding g is what we call \"training\" and when we have found it, we can use it to predict on new inputs.\n",
    "\n",
    "Within supervised learning we distinguish between __two subcategories__. If the output y is a real number we call it __regression__. If the output y is a member of a discrete set - e.g. $y \\in \\{red,blue,green\\}$ - then we call it __classification__.\n",
    "\n",
    "In Supervised Learning we try to minimize $E_{in}(g)$, but what we _really_ care about in the end is having a low out-of-sample error $E_{out}(g)$, that is, in the end we only care about how g performs on _new_ data. Since we cannot measure $E_{out}(g)$ during training, we hope that we can generalize such that $E_{in}(g)$ - which we _can_ measure - is close to $E_{out}(g)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward neural networks\n",
    "\n",
    "We shall focus on the feed forward type of neural nets. An example of such a network is shown below.\n",
    "\n",
    "![title](imgs/ffnn.png)\n",
    "\n",
    "It consists of a number of layers, each containing a number of neurons. The first layer is called the input layer, and the last is called the output layer. The layers in between are called __hidden layers__. The layers are connected and computation flows from the input layer towards the output layer. There are __weights__ associated with each of these connections. For each layer there is also a __bias__ each neuron. Some neural nets allow cycles such that there are arrows from one layer back to an earlier layer, but for a feed forward neural net the graph __must be acyclic__. The output layer and the hidden layers have a __activation function (or non-linearity)__ associated with it. Common choices are __sigmoid, tangh or ReLU__. If there is more than one or two hidden layers, then it is a __deep network (deep learning)__. If the task is __regression__, then the output layer may consist of a single neuron which may or may not have an activation function. If the task is __classification__, then the network may have k output neurons - one for each class. \n",
    "\n",
    "Neural networks are __universal representors__ meaning that for any continous function f, there is a neural net that can approximate f to _any_  degree of accuracy (might be a huge net). This tells us that the hypothesis set for neural nets is complex, and gets more complex as the size of the net grows. We could keep the complexity in check by keeping the net small, __but often it is better use increased approximation power that comes with a big net, and then make sure we dont fit too much using regularization__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural net\n",
    "\n",
    "The cost function is least squares, and for neural nets which often have thousands of parameters, this function is not generally convex. Nonetheless, we can still use gradient descent to converge to some local minimum.\n",
    "\n",
    "\n",
    "To compute all partial derivatives (with respect to all w's and b's) efficiently, the __backpropagation__ technique is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks\n",
    "\n",
    "Feature detector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
