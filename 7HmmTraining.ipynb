{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Hidden markov models - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden Markov Models\n",
    "* 3 Problems for HMM's\n",
    "    * Compute P(X|M)\n",
    "    * __Decoding__\n",
    "    * __Training__ \n",
    "* Training\n",
    "    * __Training by counting__\n",
    "    * __Viterbi training__\n",
    "    * __Expectation maximization__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hidden Markov Models\n",
    "\n",
    "In many other machine learning situations we have the assumption of __independent and identically distributed__ data. This is not always a reasonable assumption. For example with __sequential data__ such as weather observations (rainy, cloudy, sunny, etc.), the probability of seeing rain one day, is affected by which type of weather was observed the day before (and possibly further back). This leads us to consider __markov models__ in which the probability of an observation is independent of all but the most recent observations. If the probability is affected by only the previous observation, then we call it a first-order markov model, and in general we can have __i'th-order markov models__ where:\n",
    "\n",
    "$$ p(x_N \\rvert x_1,...,x_{N-1}) = p(x_n \\rvert x_{N-i},..., x_{N-1})$$\n",
    "\n",
    "If the observations are affected by latent (or hidden) discrete variables, and form a markov chain, then we have a __hidden markov model (hmm)__. For the weather observations, is not actually the outcome of the previous days, but rather its affected by low/high pressure areas.\n",
    "\n",
    "A k-state hmm can be represented by a 3-tuple of matrices $(\\pi,A,\\theta)$:\n",
    "\n",
    "-  $\\pi: k x 1$ matrix of initial state probabilities\n",
    "-  $A: k\\ x\\ k$ matrix of transition probabilities.\n",
    "-  $\\theta: k\\ x\\ |\\Sigma|$ matrix of emission probabilities\n",
    "\n",
    "Where $\\Sigma$ is the \"emission alphabet\". For the weather example it contains \"sun\", \"rain\", \"cloudy\" etc.  \n",
    "\n",
    "__A hmm generates a sequence of observables by jumping from state to state, according to A, each time emitting an observable according to $\\theta$.__\n",
    "\n",
    "An example observations sequence X, with corresponding underlying state sequence Z, can be seen below for the weather example:\n",
    "\n",
    "<img src=\"imgs\\hmm.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3 Problems for hmm's\n",
    "\n",
    "For hmm's there are 3 basic problems which must be solved for them to be useful.\n",
    "\n",
    "#### 1) Compute P(X|M) \n",
    "\n",
    "To compute $P(X\\rvert M)$, that is, the probability of seeing some sequence of observables given a hmm M. To do this we can argue that the following holds:\n",
    "\n",
    "$$P(X\\rvert M) = \\sum_{z\\in Z} P(X,Z\\rvert M)$$\n",
    "\n",
    "The probability of seeing an observation sequence is the sum of all joint probabilities with different underlying state sequences (different state sequences can produce the same sequence of observables). There are $k^N$ different state sequences possible - where k is the number of states in the hmm - so computing this directly is infeasible. Instead it can be calculated in time $O(K^2N)$ time as a bi-product of posterior decoding, by summing the last column of the $\\alpha$-table.\n",
    "\n",
    "\n",
    "##### Joint probability P(X,Z | M)\n",
    "\n",
    "The formula for computing the joint probability looks as follows:\n",
    "\n",
    "$$ P(X,Z \\rvert M) = P(z_1 \\rvert \\pi) \\bigg[\\prod_{n=2}^N P(z_n \\rvert z_{n-1}, A)\\bigg] \\prod_{n=2}^N P(x_n \\rvert z_n, \\theta)$$\n",
    "\n",
    "-  $P(z_1 \\rvert \\pi)$  is the probability of starting in state $z_1$\n",
    "-  $\\prod_{n=2}^N P(z_n \\rvert z_{n-1}, A)$ is the probability of going though the state sequence Z.\n",
    "-  $\\prod_{n=1}^N P(x_n \\rvert z_n, \\theta)$ is the emission probabilities for this particular state sequence.\n",
    "\n",
    "N is often a very large number, so the joint probability can be come _very_ small. To avoid numerical underflow one can compute $log(P(X,Z \\rvert M)$ instead.\n",
    "\n",
    "#### 2) Decoding\n",
    "\n",
    "The second basic problem for hmm's is __decoding.__ Here we are interested in finding the most likely hidden state sequence which produced a given observation sequence (__Viterbi__), or finding the most likely state to be in at a given point in the observation sequence (__Posterior__).\n",
    "\n",
    "##### Viterbi decoding\n",
    "\n",
    "In viterbi decoding we wish to find the following:\n",
    "\n",
    "$$Z^\\ast = arg \\underset{Z}{\\operatorname{max}} P(X,Z \\rvert M) $$\n",
    "\n",
    "That is, the state sequence Z that maximize the joint probability $P(X,Z \\rvert M)$. We do this in 3 steps:\n",
    "\n",
    "-  Compute the $\\omega$-table\n",
    "-  Pick the row with the largest value in the last column\n",
    "-  Backtrack to obtain optimal path\n",
    "\n",
    "##### Posterior  decoding\n",
    "\n",
    "In posterior decoding we wish to find the following:\n",
    "\n",
    "$$z^\\ast_n = arg \\underset{z_n}{\\operatorname{max}} P(z_n  \\rvert x_1,...x_N) $$\n",
    "\n",
    "That is, the most likely state to be in at the n'th step given an observation sequence.\n",
    "\n",
    "#### 3) Training\n",
    "\n",
    "Training is the third basic problem for hmm's. It is the problem of selecting model parameters $(\\pi,A,\\theta)$, to reflect given (X,Z) pair's or just a set of X's.\n",
    "\n",
    "##### Training by counting\n",
    "\n",
    "If we are given several sequences of observations and corresponding latent states - how do we set model parameters to make the given (X,Z)'s most likely to occur? The parameters should reflect what we have seen.\n",
    "\n",
    "In \"training by counting\" we count the relevant occurrences and adjust the model parameters to reflect these. This yields a __maximum likelihood estimation__ of $M = (\\pi, A, \\theta)$ by maximizing the likelihood:\n",
    "\n",
    "$$P(X\\rvert M) = \\sum_{z\\in Z} P(X,Z\\rvert M)$$\n",
    "\n",
    "##### Viterbi training\n",
    "\n",
    "If we only have the X's to work with - that is, the Z's are unknown - we can use Viterbi training. It involves 4 steps:\n",
    "\n",
    "1) Decide on initial parameters $M_0 = (\\pi_0, A_0, \\theta_0)$.\n",
    "\n",
    "2) Find the most likely sequence of states $Z^\\ast$ explaining X using the Viterbi Algorithm and the current parameters $M^i$.\n",
    "\n",
    "3) Update parameters to $M^{i+1}$ by “counting” (with pseudo counts) according to $(X,Z^\\ast)$.\n",
    "\n",
    "4) Repeat 2-3 until $P(X,Z* | M^i)$ is satisfactory (or the Viterbi sequence of states does not change).\n",
    "\n",
    "##### Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "#### Training by counting\n",
    "\n",
    "In training by counting, we iterate the data and count all relevant occurrences. For example, to set the __transition probabilities__, we can look at the Z's and count how many times the transition $s_i \\rightarrow s_j$ occurs for two states $s_i$ and $s_j$, and set the probability $A[i,j]$ to be the fraction of these transitions divided by the total amount of transitions from $s_i$ to _any_ state. i.e:\n",
    "\n",
    "$$A[i,j] =\\frac{count_{s_i \\rightarrow s_j}}{count_{s_i \\rightarrow any}}$$\n",
    "\n",
    "To set the __initial state probabilities__ $\\pi$, we simply look at the first state in each Z sequence, and update the entries in $\\pi$ accordingly. \n",
    "\n",
    "To set the __emission probabilities__ $\\theta$, we have to look at both the X's and the Z's. We count the number of times a state $s_i$ emits an observable $o_j$, and update $\\theta(i,j)$ to be the number of these emissions divided by the total amount of emissions from $s_i$. i.e:\n",
    "\n",
    "$$\\theta(i,j) = \\frac{count_{s_i\\ emit:\\ o_j}}{count_{s_i\\ emit:\\ any}}$$\n",
    "\n",
    "#### Viterbi training\n",
    "\n",
    "\n",
    "#### Expectation Maximizatin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
