{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6) Hidden Markov Models - Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden Markov Models\n",
    "* 3 Problems for HMM's\n",
    "    * Compute P(X|Y)\n",
    "    * __Decoding__\n",
    "    * __Training__  \n",
    "* Decoding\n",
    "    * __Viterbi__ \n",
    "    * __Posterior__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models\n",
    "\n",
    "In many other machine learning situations we have the assumption of __independent and identically distributed__ data. This is not always a reasonable assumption. For example with __sequential data__ such as weather observations (rainy, cloudy, sunny, etc.), the probability of seeing rain one day, is affected by which type of weather was observed the day before (and possibly further back). This leads us to consider __markov models__ in which the probability of an observation is independent of all but the most recent observations. If the probability is affected by only the previous observation, then we call it a first-order markov model, and in general we can have __i'th-order markov models__ where:\n",
    "\n",
    "$$ p(x_N \\rvert x_1,...,x_{N-1}) = p(x_n \\rvert x_{N-i},..., x_{N-1})$$\n",
    "\n",
    "If the observations are affected by latent (or hidden) discrete variables, and form a markov chain, then we have a __hidden markov model (hmm)__. For the weather observations, is not actually the outcome of the previous days, but rather its affected by low/high pressure areas.\n",
    "\n",
    "A k-state hmm can be represented by a 3-tuple of matrices $(\\pi,A,\\theta)$:\n",
    "\n",
    "-  $\\pi: k x 1$ matrix of initial state probabilities\n",
    "-  $A: k\\ x\\ k$ matrix of transition probabilities.\n",
    "-  $\\theta: k\\ x\\ |\\Sigma|$ matrix of emission probabilities\n",
    "\n",
    "Where $\\Sigma$ is the \"emission alphabet\". For the weather example it contains \"sun\", \"rain\", \"cloudy\" etc.  \n",
    "\n",
    "__A hmm generates a sequence of observables by jumping from state to state, according to A, each time emitting an observable according to $\\theta$.__\n",
    "\n",
    "An example observations sequence X, with corresponding underlying state sequence Z, can be seen below for the weather example:\n",
    "\n",
    "<img src=\"imgs\\hmm.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Problems for hmm's\n",
    "\n",
    "For hmm's there are 3 basic problems which must be solved for them to be useful.\n",
    "\n",
    "#### 1) Compute P(X|M) \n",
    "\n",
    "To compute $P(X\\rvert M)$, that is, the probability of seeing some sequence of observables given a hmm M. To do this we can argue that the following holds:\n",
    "\n",
    "$$P(X\\rvert M) = \\sum_{z\\in Z} P(X,Z\\rvert M)$$\n",
    "\n",
    "The probability of seeing an observation sequence is the sum of all joint probabilities with different underlying state sequences (different state sequences can produce the same sequence of observables). There are $k^N$ different state sequences possible - where k is the number of states in the hmm - so computing this directly is infeasible. Instead it can be calculated in time $O(K^2N)$ time as a bi-product of posterior decoding, by summing the last column of the $\\alpha$-table.\n",
    "\n",
    "\n",
    "##### Joint probability P(X,Z | M)\n",
    "\n",
    "The formula for computing the joint probability looks as follows:\n",
    "\n",
    "$$ P(X,Z \\rvert M) = P(z_1 \\rvert \\pi) \\bigg[\\prod_{n=2}^N P(z_n \\rvert z_{n-1}, A)\\bigg] \\prod_{n=2}^N P(x_n \\rvert z_n, \\theta)$$\n",
    "\n",
    "-  $P(z_1 \\rvert \\pi)$  is the probability of starting in state $z_1$\n",
    "-  $\\prod_{n=2}^N P(z_n \\rvert z_{n-1}, A)$ is the probability of going though the state sequence Z.\n",
    "-  $\\prod_{n=1}^N P(x_n \\rvert z_n, \\theta)$ is the emission probabilities for this particular state sequence.\n",
    "\n",
    "N is often a very large number, so the joint probability can be come _very_ small. To avoid numerical underflow one can compute $log(P(X,Z \\rvert M)$ instead.\n",
    "\n",
    "#### 2) Decoding\n",
    "\n",
    "The second basic problem for hmm's is __decoding.__ Here we are interested in finding the most likely hidden state sequence which produced a given observation sequence (__Viterbi__), or finding the most likely state to be in at a given point in the observation sequence (__Posterior__).\n",
    "\n",
    "##### Viterbi decoding\n",
    "\n",
    "In Viterbi decoding we wish to find the following:\n",
    "\n",
    "$$Z^\\ast = arg \\underset{Z}{\\operatorname{max}} P(X,Z \\rvert M) $$\n",
    "\n",
    "That is, the state sequence Z that maximize the joint probability $P(X,Z \\rvert M)$. We do this in 3 steps:\n",
    "\n",
    "-  Compute the $\\omega$-table\n",
    "-  Pick the row with the largest value in the last column\n",
    "-  Backtrack to obtain optimal path\n",
    "\n",
    "##### Posterior  decoding\n",
    "\n",
    "In Posterior decoding we wish to find the following:\n",
    "\n",
    "$$z^\\ast_n = arg \\underset{z_n}{\\operatorname{max}} P(z_n  \\rvert x_1,...x_N) $$\n",
    "\n",
    "That is, the most likely state to be in at the n'th step given an observation sequence.\n",
    "\n",
    "#### 3) Training\n",
    "\n",
    "##### Training by counting\n",
    "\n",
    "##### Viterbi training\n",
    "\n",
    "##### Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "\n",
    "#### Viterbi decoding \n",
    "\n",
    "That is, the state sequence Z that maximize the joint probability $P(X,Z \\rvert M)$. To find this $Z^\\ast$ we first compute the $(k\\ x\\ N)$-sized $\\omega$-table where the entry $\\omega(z_n)$ is the probability of the most likely sequence of states $z_1,z_2,...,z_n$, ending in $z_n$ - that generated the observations $x_1, x_2,...,x_n$. When we have computed the table, we can pick the row in the last colum with the largest value\n",
    "w\n",
    "- given sequence of observations X, what is the most likely explanation  Z (hidden state sequence)?\n",
    "\n",
    "#### Posterior decoding\n",
    "- Given X of length n, whats most likely state to be in at time n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
