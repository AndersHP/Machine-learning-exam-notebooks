{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Unsupervised learning - Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unsupervised learning (briefly)\n",
    "* Outlier detection\n",
    "* Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "In unsupervised learning the training data has the form $D = \\{(x_1, ?), (x_2, ?),...(x_n, ?)\\}$. So we have no labels for the data pwoints, and the goal is to get an insigt into the data distribution, and see if we can make some sense of it. __Three important problems__ are __clustering__, where we applying algorithms to try and find natural clusters within the data, and __outlier detection__, where we try to indentity data points which seem to deviate from the pattern in the data, and finally __dimensionality reduction__ where we seek to reduce the dimensionality of our data while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection\n",
    "\n",
    "Outlier detection is in a sense the opposite problem of clustering. In clustering we want to find the points that are similar and group them. In outlier detection we seek to find the points that are not similar to other points.\n",
    "\n",
    "The standard definition of an __outlier__ is that is an object that deviates so much from the rest of the data set as to\n",
    "arouse suspicion that it was generated by a different mechanism. It could be that the point is just an error, and so we might want to remove it. It could also indicate fraud, or simply be a valid but rare point. An outlier can be both an abnomality or simply just noise. Often the abnormalies can have high interest to the analyst.\n",
    "\n",
    "Outlier detection can be applied to areas such as:\n",
    "\n",
    "- Credit card fraud detection\n",
    "- Telecom fraud detection \n",
    "- Customer segmentation\n",
    "- Medical analysis\n",
    "- Surveillance\n",
    "\n",
    "The output of an outlier algorithm can be binary labels, or a score for how much an object is an outlier. \n",
    "\n",
    "Outlier detection can be done using supervised, unsupervised or semi-supervised approaches, but here we discuss unsupervised approaches\n",
    "\n",
    "#### DBScan\n",
    "\n",
    "Dbscan finds outliers as a bi-product. (the points that does not get in a cluster).\n",
    "\n",
    "#### Basic outlier detection models\n",
    "\n",
    "\n",
    "#### Probabilistic and  statistical approach\n",
    "\n",
    "Often first step before smarter things.\n",
    "\n",
    "Limitations: Only for single features. Sensitive to many outliers, because outliers affect learning. Not all outliers are detected\n",
    "\n",
    "#### Expectation maximization\n",
    "\n",
    "Fit several gaussians to the data. \n",
    "\n",
    "\n",
    "### Local outliers instead of global outliers\n",
    "\n",
    "Each object has an outlier factor: degree to which the object deviates. \n",
    "\n",
    "k-distance of an object p:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "The goal of dimensionality reduction is to take high dimensional data, and reduce the number of dimensions while preserving as much information/structure as possible. If we have very high dimensional data, there might be some redundant attributes, and if two attributes \"change together\" and are almost linearly dependent, then discarding one of them doesnt cost us much in terms of information/structure (we can always infer the discarded one from the one we keep).\n",
    "\n",
    "The motivations for doing dimensionality reduction include the following. In terms of generalization, lowering the dimension will lower the complexity of the chosen hypothesis, which helps combat overfitting. Many dimensional data also require more computation than lower dimension data. Reducing to two or three dimensions is often used to facilitate visualization of higher dimensional data.\n",
    "\n",
    "\n",
    "#### Principle Components Analysis (PCA)\n",
    "\n",
    "Assumption: Each observed variable should be normally distributed\n",
    "\n",
    "PCA is a technique for reducing the number of dimensions. The idea is to find the unit vectors $u_i$ which point in the direction of highest variance. Picking these directions preserve the relative distance between the data points, and therefore preserve more information. These directions are called __principal components__, and if we want to reduce the dimensionality from d to k, we pick the k principal components $u_1,...,u_k$, and compute a new representation $x_i'$ for each point $x_i$ as:\n",
    "\n",
    "\n",
    "\n",
    "$$ x_i' =\\begin{bmatrix}\n",
    "           u_1^T x_{i} \\\\\n",
    "           u_2^T x_{i} \\\\\n",
    "           \\vdots \\\\\n",
    "           u_k^T x_{i}\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^k\n",
    "$$\n",
    "\n",
    "Here each of the k entries in $x_i'$ represent $x_i$'s projection onto the principle component unit vector $u_j$. Since the principle components are unit vectors, this projection will give us the distance from the mean to the point (along the direction of $u_j$).\n",
    "\n",
    "<img src=\"imgs/projection.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "#### Finding the principal components\n",
    "\n",
    "The top principle component is given by: \n",
    "\n",
    "$$\\underset{u}{ \\operatorname{arg max}} \\frac{1}{n}\\sum_{i=1}^n(x_i^T u)^2$$\n",
    "\n",
    "That is, it is the vector u that maximize the average squared length of projections of data onto u. Further we constrain the lenght of u to be 1. This formula can be rewritten as:\n",
    "\n",
    "$$ \\frac{1}{n}\\sum_{i=1}^n(x_i^T u)^2 = \\frac{1}{n}\\sum_{i=1}^n(x_i^T u)^T(x_i^T u) = u^T \\bigg[ \\frac{1}{n}\\sum_{i=1}^n x_i x_i^T \\bigg] u$$ \n",
    "\n",
    "Here we can identify the term in big square brackets as the covariance matrix (notice that we dont subtract the mean because we assume the data has been normalized so mean = 0, see section on preprocessing). __Linear algebra tells us that the vector u that maximize this expression is the largest eigenvector of the covariance matrix__. Hence finding the principal component is a matter of finding the eigenvectors of the covariance matrix.\n",
    "\n",
    "The covariance matrix tells us something about the elipsioidal shape of the data. See the plots below of 2d data with different covariance matrices. From the plots its easy to see which direction the principle component points. \n",
    "\n",
    "<img src=\"imgs/covintuition.png\" style=\"width: 450px;\"/>\n",
    "http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/\n",
    "\n",
    "\n",
    "#### To sum up\n",
    "\n",
    "$$\\Sigma u = \\lambda u$$\n",
    "\n",
    "0) Normalize the data (see preprocessing section) \n",
    "\n",
    "1) Compute covariance matrix\n",
    "\n",
    "2) Find eigenvectors/values and pick the longer eigenvector(and normalize them)\n",
    "\n",
    "The covariance matrix can be computed using the __outer product__ as follows:\n",
    "\n",
    "$$Cov(X) = \\frac{1}{n}\\sum_{i=1}^n (x_i- \\bar x) (x_i- \\bar x)^T$$\n",
    "\n",
    "Notice that $(x_i- \\bar x)$ is a d dimensional column vector corresponding to a data point $x_i$ where the mean of each attribute is subtracted from the corresponding entry. Note also that if we have preprocessed the data, then all means are 0, so we dont have to subtract it.\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "#### Preprocessing data\n",
    "\n",
    "To compare the variances of each dimension fairly we must normalize the data, such that each dimension have mean = 0 and variance = 1 (attributes with high values like yearly salary have much higher variance than attributes with low values like years of employment). \n",
    "\n",
    "We center the data (mean = 0) by subtracting the mean $\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$ from each attribute.\n",
    "\n",
    "We normalize the variance by dividing each attribute by the variance $\\sigma^2  = \\frac{1}{n}\\sum_{i=1}^n(x_i-\\mu)^2$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
