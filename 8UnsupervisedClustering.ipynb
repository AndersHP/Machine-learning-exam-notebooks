{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Unsupervised learning - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unsupervised learning (briefly)\n",
    "* Clustering  \n",
    "    * Representative-based clustering\n",
    "    * Density-based clustering\n",
    "    * Hierarchical clustering\n",
    "    * Subspace clustering\n",
    "* K-means\n",
    "* Expectation-maximization clustering\n",
    "* Cluster measurement\n",
    "    * Silhouette coefficient\n",
    "    * F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "In unsupervised learning the training data has the form $D = \\{(x_1, ?), (x_2, ?),...(x_n, ?)\\}$. So we have no labels for the data points, and the goal is to get an insigt into the data distribution, and see if we can make some sense of it. __Two important problems__ are __clustering__, where we applying algorithms to try and find natural clusters within the data, and __outlier detection__, where we try to indentity data points which seem to deviate from the pattern in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Clustering\n",
    "\n",
    "Clustering can be though of as organizing the data into \"natural\" clusters. Natural in the sense that members of the same cluster are similar/close to each other. Clustering can be used as a preprocessing step for supervised learning, to find groups in the data and assign labels to them.\n",
    "\n",
    "<img src=\"imgs/clustering.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "There are several clustering approaches:\n",
    "\n",
    "#### Representative-based clustering\n",
    "\n",
    "The goal of representative-based clustering is to partition the dataset into k clusters where each cluster is represented by some point. For some algorithms the point is an actual point in the data set, but sometimes it is just the mean of the points.\n",
    "\n",
    "The representative-based clustering methods finds convex-shaped clusters, so when the clusters have nonconvex structure they perform poorly. Furthermore the number of clusters k must be specified by the user, which often not possible. \n",
    "\n",
    "#### Density-based clustering\n",
    "\n",
    "Density-based approaches look at the density of the neighborhood around each point, and choose clusters accordingly. These approaches determine the value of k, so the user does not have to specify then number of clusters as is the case in representative-based clustering. Of course this does not imply that the algorithm will \"detect\" the correct number of clusters. Instead of the parameter k, the user has to provide parameters $\\epsilon$ and minpts. These parameters specify what level of density is enough to determine a cluster, and thus have huge impact on the result. Density-based clustering does not care if the clusters are convex or not. \n",
    "\n",
    "Contrary to representative-based algorithms these types of algorithm can detect noise (points not in a cluster) and this i is related to outlier detection.\n",
    "\n",
    "\n",
    "#### Hierachical clustering\n",
    "\n",
    "\n",
    "\n",
    "In hierachical clustering we wish to build a dendrogram(tree) of subclusters. The root of the tree consists of a cluster containing all data points, and the leaves contain clusters containing a single point. Thus, the meaningful clusters lie somewhere in the other layers of the tree. The trees can be constructed bottom-up (called agglomerative approach) or top-down (divisive approach). Sometimes different clusters have different density, in such cases density-based clustering algorithms struggle, because of the fixed parameters $\\epsilon$ and minpts. Thi\n",
    "\n",
    "#### Subspace clustering\n",
    "\n",
    "If our data is high dimensional, there is a risk that some of the features are irrelevant and can thus obscure an otherwise good clustering. This is illustrated below where the data in 3d looks random, but if one dimension is discarded it looks like there is some structure. \n",
    "\n",
    "<img src=\"imgs/subspaceclustering.png\" style=\"width: 450px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "The most well known clustering algorithm is the K-means algorithm. Here we want to split the dataset into k clusters $C = \\{c_1,...c_k\\}$, each represented by a centroid $\\mu_i$, such that the following error function is minimized:\n",
    "\n",
    "$$SSE(C) = \\sum_{i=1}^k \\sum_{x_j \\in c_i} \\|x_j - \\mu_i \\|^2$$\n",
    "\n",
    "This is called _sum of squared errors_ (sum of all squared distances from all points to their centroid $\\mu$) and the algorithm iteratively tries to minimize this sum. It does so by repeatedly assigning the data points to the cluster defined by the nearest centroid, and recalculating the centroids of the new clusters. The algorithm is described in pseudocode below:\n",
    "\n",
    "<img src=\"imgs/kmeans.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "The stopping criterion is given by:\n",
    "\n",
    "$$\\sum_{i=1}^k \\| \\mu_i^t - \\mu_i^{t-1}\\|^2 \\le \\epsilon$$\n",
    "\n",
    "The running time of the algorithm is $ \\mathcal{O}(tknd)$ where n is \\#points, k is \\#clusters, t is \\# iterations, and d is the dimensionality of the data points.\n",
    "\n",
    "The k-mean approach to clustering have the following __advantages__:\n",
    "\n",
    "- Relatively efficient (often k,t,d << n)\n",
    "- Easy to understand and implement\n",
    "\n",
    "And the following __disadvantages__:\n",
    "\n",
    "- Only works if mean is defined (cannot compute mean of categorical values like (1=red, 2=green etc))\n",
    "- Hard to know what k to pick\n",
    "- Sensitive to noisy data and outliers\n",
    "- Clusters will always have convex shapes\n",
    "- All points get to be in a cluster (shouldnt always be the case)\n",
    "- Can terminate in local minima\n",
    "\n",
    "For the problem of deciding the number of clusters (k), we could try all k from 1 to n-1, and pick the one with the least error. This would be the clustering with k-1, but it's unlikely that this clustering makes any sense. __a measurement of how good a clustering is must be independent of k__ (see silhouette coefficient section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization \n",
    "\n",
    "K-means clustering provide _hard assignment_ clusterings. That is, for each point we report: _this_ is the cluster it belongs to. Expectation Maximization(EM) clustering provides a __soft assignment__ clustering, where each point has a probability of belonging i each cluster. Each cluster is represented by a gaussian distribution, and so if we require a hard assignment, we can just report the cluster which have the highest probability of containing that point. Each gaussian has a mean and a covariance matrix which describe the clusters elipsoidal shape.\n",
    "\n",
    "\n",
    "\n",
    "The density function for each cluster C is the standard gaussian multivariate density function:\n",
    "\n",
    "$$P(x\\rvert C) = \\frac{1}{\\sqrt{(2\\pi)^d\\rvert \\Sigma_C \\rvert}}\\cdot e^{-\\frac{1}{2}(x-\\mu_C)^T\\cdot(\\Sigma_C)^{-1}\\cdot(x-\\mu_C)}$$\n",
    "\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "For each cluster $C_i$ we __initialize a random mean__ within the ranges of each dimension. We then __initialize the covariance matrix to be the identity matrix__. Finally we __initialize the prior probabilities, $P(C_i)$, to $\\frac{1}{k}$__, such that each cluster has equal probability from the beginning.\n",
    "\n",
    "##### E step\n",
    "\n",
    "\n",
    "In each expectation step we compute the __posterior probability__ of a cluster $C_i$ given a point $x$, $P(C_i \\rvert x)$. This probability can be thought of as a __weight or contribution of point x to cluster $C_i$__, and will be used in the M step. We use bayes theorem to compute posteriors:\n",
    "\n",
    "$$ P(C_i \\rvert x) = \\frac{P(C_i, x)}{P(x)} = \\frac{P(x \\rvert C_i) P(C_i)}{\\sum_{a = 1}^k P(x \\rvert C_i) P(C_i)} $$\n",
    "\n",
    "##### M step\n",
    "\n",
    "In each maximization step we re-estimate 3 things:\n",
    "\n",
    "__Priors__ $P(C_i)$ for each cluster as the fraction of weights that contribute to that cluster:\n",
    "\n",
    "$$P(C_i) = \\frac{1}{n}\\sum_{x\\in D}P(C_i \\rvert x)$$\n",
    "\n",
    "\n",
    "__Means__ $\\mu_i$ of $C_i$, as the weighted averate of all points:\n",
    "\n",
    "\n",
    "$$\\mu_i = \\frac{\\sum_{x\\in D} x \\cdot P(C_i \\rvert x)}{\\sum_{x\\in D} P(C_i \\rvert x)} $$\n",
    "\n",
    "__Covariance matrix__ $\\Sigma_i$ as the weighted covariance over all pairs of dimensions:\n",
    "\n",
    "\n",
    "$$\\Sigma_i = \\frac{\\sum_{x\\in D} P(C_i \\rvert x)(x-\\mu_i)(x-\\mu_i)^T}{\\sum_{x\\in D} P(C_i \\rvert x)}$$\n",
    "\n",
    "##### Stopping criterion\n",
    "\n",
    "We can stop the algorithm when the means no longer change significantly (same stopping criterion as in k-means). The algorithm will then have converged to a local maximum of the log likelihood. And the parameters for the gaussians now constitute a (local)__maximum likelihood estimation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster measurement\n",
    "\n",
    "It is important to be able to measure how well a clustering is. There are two overall categories, __internal__ and __external__ evaluation mearsures. External measures assume that the \"correct\" clustering is known beforehand, that is, the labels are given for all data points. If we know the correct clustering beforehand, there is not need for clustering, but instead the labels can be used to test and validate clustering methods. In most cases however we dont know the labels, and so to evaluate the quality of a clustering internal measurements must measure the compactness and \"similarity\" between objects in the same cluster.\n",
    "\n",
    "\n",
    "#### Silhouette coefficient\n",
    "\n",
    "The silhouette coefficient approach is an __internal evaluation measure__, and it provides measurements of both how well each object belongs to its assigned cluster, and how good the total clustering is.\n",
    "Let $a(o)$ be the average distance between object o and other objects in its cluster A, and let $b(o)$ be the average distance from object o to all objects _in its second closest cluster_. Formally:\n",
    "\n",
    "$$\n",
    "a(o) = \\frac{1}{\\vert C_A \\vert} \\sum_{p\\in C_A} dist(o,p)\\\\\n",
    "b(o) = \\underset{C_i \\neq C_A}{\\operatorname{min}} \\bigg[ \\frac{1}{\\vert C_i \\vert} \\sum_{p\\in C_i} dist(o,p) \\bigg]\n",
    "$$\n",
    "\n",
    "The silhouette of an object o is then given by:\n",
    "\n",
    "$$s(o) = \\frac{b(o) - a(o)}{max\\{a(o),b(o)\\}}$$\n",
    "\n",
    "Its easy to see that if the $b(o)$ is very big, and  $a(o)$ is very small (which sound ideal), then the numerator and denominator are close to each other, and the $s(o)$ will be close to 1. On the other hand, if $b(o)$ is small and $a(o)$ is big (which sound bad), then the numerater is negative, and the denominator is positive, and so $s(o)$ will be close to -1. Hence object o seems to be in the correct cluster if $s(o)$ is close to 1, and in the wrong cluster if $s(o)$ is close to -1.\n",
    "\n",
    "<img src=\"imgs/silhouette.png\" style=\"width: 350px;\"/>\n",
    "\n",
    "The silhouette coefficient $s_C$ of a clustering is the average of all $s(o)$'s. This value is an indication of how strong the stucture is. If $s_C$ is between 0.7 and 1, then we say that the clustring have strong structure. Below 0.5 is weak to no structure. Often we use the technique to compare clusters - for example to determine the parameter k - so we don't care what the value is, just which clustering scores highest.\n",
    "\n",
    "\n",
    "#### F1 score\n",
    "\n",
    "F1 score is an __external evaluation measure__, and thus it is useful only when we have the set of correct labels for the data points. \n",
    "\n",
    "TODO mere\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
