{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6) Hidden Markov Models - Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden Markov Models\n",
    "* 3 Problems for HMM's\n",
    "    * Compute P(X|Y)\n",
    "    * __Decoding__\n",
    "    * __Training__  \n",
    "* Decoding\n",
    "    * __Viterbi__ \n",
    "    * __Posterior__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models\n",
    "\n",
    "In many other machine learning situations we have the assumption of __independent and identically distributed__ data. This is not always a reasonable assumption. For example with __sequential data__ such as weather observations (rainy, cloudy, sunny, etc.), the probability of seeing rain one day, is affected by which type of weather was observed the day before (and possibly further back). This leads us to consider __markov models__ in which the probability of an observation is independent of all but the most recent observations. If the probability is affected by only the previous observation, then we call it a first-order markov model, and in general we can have __k-order markov models__ where:\n",
    "\n",
    "$$ p(x_n \\rvert x_1,...,x_{n-1}) = p(x_n \\rvert x_{n-k},..., x_{n-1})$$\n",
    "\n",
    "If the observations are affected by latent (or hidden) discrete variables, and form a markov chain, then we have a __hidden markov model__. For the weather observations, is not actually the outcome of the previous days, but rather its affected by low/high pressure areas.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Problems for HMM's\n",
    "\n",
    "#### 1) Compute P(X|Y)\n",
    "\n",
    "#### 2) Decoding\n",
    "\n",
    "#### 3) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "\n",
    "#### Viterbi decoding \n",
    "- given sequence of observations X, what is the most likely explanation  Z (hidden state sequence)?\n",
    "\n",
    "#### Posterior decoding\n",
    "- Given X of length n, whats most likely state to be in at time n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
