{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Unsupervised learning - Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unsupervised learning (briefly)\n",
    "* Outlier detection\n",
    "* Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "In unsupervised learning the training data has the form $D = \\{(x_1, ?), (x_2, ?),...(x_n, ?)\\}$. So we have no labels for the data pwoints, and the goal is to get an insigt into the data distribution, and see if we can make some sense of it. __Three important problems__ are __clustering__, where we applying algorithms to try and find natural clusters within the data, and __outlier detection__, where we try to indentity data points which seem to deviate from the pattern in the data, and finally __dimensionality reduction__ where we seek to reduce the dimensionality of our data while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection\n",
    "\n",
    "The standard definition of an __outlier__ is that is an object that deviates so much from the rest of the data set as to\n",
    "arouse suspicion that it was generated by a different mechanism. It could be that the point is just an error, and so we might want to remove it. It could also indicate fraud, or simply be a valid but rare point. An outlier can be both an abnomality or simply just noise. Often the abnormalies can have high interest to the analyst.\n",
    "\n",
    "Outlier detection can be applied to areas such as:\n",
    "\n",
    "- Credit card fraud detection\n",
    "- Telecom fraud detection \n",
    "- Customer segmentation\n",
    "- Medical analysis\n",
    "- Surveillance\n",
    "\n",
    "The output of an outlier algorithm can be binary labels, or a score for how much an object is an outlier. \n",
    "\n",
    "#### Basic outlier detection models\n",
    "\n",
    "\n",
    "\n",
    "#### Probabilistic and  statistical approach\n",
    "\n",
    "Often first step before smarter things.\n",
    "\n",
    "Limitations: Only for single features. Sensitive to many outliers, because outliers affect learning. Not all outliers are detected\n",
    "\n",
    "#### Expectation maximization\n",
    "\n",
    "#### Density based clustering for outlier detection\n",
    "\n",
    "FOr dbscan. All objects not in cluster are outliers\n",
    "\n",
    "\n",
    "### Local outliers instead of global outliers\n",
    "\n",
    "Each object has an outlier factor: degree to which the object deviates. \n",
    "\n",
    "k-distance of an object p:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction\n",
    "\n",
    "The goal of dimensionality reduction is to take high dimensional data, and reduce the number of dimensions while preserving as much information/structure as possible. If we have very high dimensional data, there might be some redundant attributes, and if two attributes \"change together\" and are almost linearly dependent, then discarding one of them doesnt cost us much in terms of information/structure (we can always infer the discarded one from the one we keep).\n",
    "\n",
    "The motivations for doing dimensionality reduction include the following. In terms of generalization, lowering the dimension will lower the complexity of the chosen hypothesis, which helps combat overfitting. Many dimensional data also require more computation than lower dimension data. Reducing to two or three dimensions is often used to facilitate visualization of higher dimensional data.\n",
    "\n",
    "\n",
    "#### Principle Components Analysis (PCA)\n",
    "\n",
    "Assumption: Each observed variable should be normally distributed\n",
    "\n",
    "PCA is a technique for reducing the number of dimensions. The idea is to find the unit vectors $u_i$ which point in the direction where the variance of the data (structure) is greatest. Picking these directions preserve the relative distance between the data points, and therefore preserve more information. These directions are called __principal components__, and if we want to reduce the dimensionality from d to k, we pick the k principal components $u_1,...,u_k$, and compute a new representation $x_i'$ for each point $x_i$ as:\n",
    "\n",
    "\n",
    "\n",
    "$$ x_i' =\\begin{bmatrix}\n",
    "           u_1^T x_{i} \\\\\n",
    "           u_2^T x_{i} \\\\\n",
    "           \\vdots \\\\\n",
    "           u_k^T x_{i}\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^k\n",
    "$$\n",
    "\n",
    "Here each of the k entries in $x_i'$ represent $x_i$'s projection onto the principle component unit vector $u_j$. Since the principle components are unit vectors, this projection will give us the distance from the mean to the point (along the direction of $u_j$).\n",
    "\n",
    "<img src=\"imgs/projection.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Finding the principal components is a matter of finding the eigenvectors of the covariance matrix $\\Sigma$. The covariance matrix tells us something about the elipsioidal shape of the data. See the plots below of 2d data with different covariance matrices. From the plots its easy to see which direction the principle component points. \n",
    "\n",
    "<img src=\"imgs/covintuition.png\" style=\"width: 450px;\"/>\n",
    "http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/\n",
    "\n",
    "__If we take a random vector and multiply it with the covariance vector, the result is a new vector that has \"turned\" towards the direction of highest variance. If we multiply the new vector with the covariance matrix, we get a new vector which points even more in direction of most variance.__\n",
    "\n",
    "#### Finding the principle components\n",
    "\n",
    "$$\\Sigma u = \\lambda u$$\n",
    "\n",
    "1) Find eigenvalues by solving: $det(\\Sigma-\\lambda I) = 0\n",
    "\n",
    "2) Find corresponding eigenvectors by putting the eigenvalues into the equation above.\n",
    "\n",
    "3) Normalize the vectors\n",
    "\n",
    "4) Sort the eigenvalue/normalized eigenvector pairs by the largest eigenvalue.\n",
    "\n",
    "5) The resulting list is the list of the first, second, third... principle components\n",
    "\n",
    "...\n",
    "\n",
    "The principle component is given by: \n",
    "\n",
    "$$\\underset{u}{ \\operatorname{arg max}} \\frac{1}{n}\\sum_{i=1}^n(u^T x_i)^2$$\n",
    "\n",
    "That is, it is the vector u that maximize the average squared length of projection of data onto u. Further we constrain the lenght of u to be 1.  \n",
    "\n",
    "\n",
    "#### Preprocessing data\n",
    "\n",
    "To compare the variances of each dimension fairly we must normalize the data, such that each dimension have mean = 0 and variance = 1 (attributes with high values like yearly salary have much higher variance than attributes with low values like years of employment). \n",
    "\n",
    "We center the data (mean = 0) by subtracting the mean $\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$ from each attribute.\n",
    "\n",
    "We normalize the variance by dividing each attribute by the variance $\\sigma^2  = \\frac{1}{n}\\sum_{i=1}^n(x_i-\\mu)^2$.\n",
    "\n",
    "\n",
    "#### 2 dimensional example\n",
    "\n",
    "First preprocess the data such that it has 0 mean and variance 1. Compute covariance matrix SIGMA. Take eigendecomposition of sigma. Sort by largest eigenvalue. Now the vector corresponding to the largest eigenvalue is the first principle component. etc. We can then pick the k principle components (where k is the new dimensionality we require)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
