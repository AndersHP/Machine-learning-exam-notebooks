{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supervised learning (briefly)\n",
    "* Max margin intuition \n",
    "* Func. & Geo. margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "In the Supervised Learning setting we are given a dataset $D = \\{(x_1,y_1),...,(x_n,y_n)\\}$ to learn from. The way we \"learn\" is by using D to search a hypothesis space H for some \"best\" hypothesis $g\\in H$. By \"best\", we mean that we want g to be as close to the __ unknown target function f__ as possible, i.e.: \n",
    "\n",
    "$$g \\approx f$$\n",
    "\n",
    "We most often find g by minimizing an in-sample error function $E_{in}(h)$. The process of finding g is what we call \"training\" and when we have found it, we can use it to predict on new inputs.\n",
    "\n",
    "Within supervised learning we distinguish between __two subcategories__. If the output y is a real number we call it __regression__. If the output y is a member of a discrete set - e.g. $y \\in \\{red,blue,green\\}$ - then we call it __classification__.\n",
    "\n",
    "In Supervised Learning we try to minimize $E_{in}(g)$, but what we _really_ care about in the end is having a low out-of-sample error $E_{out}(g)$, that is, in the end we only care about how g performs on _new_ data. Since we cannot measure $E_{out}(g)$ during training, we hope that we can generalize such that $E_{in}(g)$ - which we _can_ measure - is close to $E_{out}(g)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "#### Fat margins constraint lowers vc dimensions in a sense\n",
    "\n",
    "9 min : https://www.youtube.com/watch?v=eHsErlPJWUU&t=138s\n",
    " \n",
    " \n",
    " #### Lagrange multipliers purpose:\n",
    " \n",
    " Maximize a function subject to some constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
