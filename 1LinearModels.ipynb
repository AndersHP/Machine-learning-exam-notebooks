{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supervised learning (briefly)\n",
    "* Perceptron (briefly)\n",
    "* Linear Regression\n",
    "    * Hypothesis set\n",
    "    * Error function\n",
    "    * Formula for $w^\\ast$\n",
    "* Logistic Regression (classificaion)\n",
    "    * Hypothesis set\n",
    "    * Sigmoid function\n",
    "    * Error function\n",
    "    * Gradient descent\n",
    "    * Softmax \n",
    "* Non-linear transforms (briefly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "In the Supervised Learning setting we are given a dataset $D = \\{(x_1,y_1),...,(x_n,y_n)\\}$ to learn from. The way we \"learn\" is by using D to search a hypothesis space H for some \"best\" hypothesis $g\\in H$. By \"best\", we mean that we want g to be as close to the __ unknown target function f__ as possible, i.e.: \n",
    "\n",
    "$$g \\approx f$$\n",
    "\n",
    "We most often find g by minimizing an in-sample error function $E_{in}(h)$. The process of finding g is what we call \"training\" and when we have found it, we can use it to predict on new inputs.\n",
    "\n",
    "Within supervised learning we distinguish between __two subcategories__. If the output y is a real number we call it __regression__. If the output y is a member of a discrete set - e.g. $y \\in \\{red,blue,green\\}$ - then we call it __classification__.\n",
    "\n",
    "In Supervised Learning we try to minimize $E_{in}(g)$, but what we _really_ care about in the end is having a low out-of-sample error $E_{out}(g)$, that is, in the end we only care about how g performs on _new_ data. Since we cannot measure $E_{out}(g)$ during training, we hope that we can generalize such that $E_{in}(g)$ - which we _can_ measure - is close to $E_{out}(g)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "The perceptron is the simplest of the linear models. It is a binary __classification__ model ({-1,+1}), and the hypothesis set looks as follows:\n",
    "\n",
    "$$ H=\\{h(x)=\\text{sign}(w^T x) \\mid w\\in \\mathbb{R}^{d+1}\\}$$\n",
    "\n",
    "The __Perceptron Learning Algorithm(PLA)__ search for a hyperplane which separate all datapoints correctly. While a misclassified point x exists, the algorithm will update the hypothesis, effectively moving the hyperplane in the direction of correctly classifying x. The algorithm never terminate if the given data is not linearly separable, but if the data is linearly separable, it is guaranteed to pick a $g\\in H$ with $E_{in} = 0$. \n",
    "\n",
    "The error function we wish to minimize is:\n",
    "\n",
    "$$E_{in}(h)=\\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{h(x_i)\\neq y_i} $$\n",
    "\n",
    "Where $\\mathbb{1}_{h(x_i)\\neq y_i} = 1$ if h misclassifies $x_i$, and 0 if not.  \n",
    "\n",
    "With a simple augmentation, the PLA can be used on non linearly separable datasets. The idea is to keep the currently best h \"in a pocket\", while runing for i iterations(instead of until $E_{in} =0$). After the i iterations, the algorithm stops and returns the stored hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "In Linear Regression, we are in the __regression__ setting. We wish to find the hyperplane which best explain the observed datapoints in D. This hyperplane can then be used to output real numbers based on new input. The hypothesis set looks as follows:\n",
    "\n",
    "$$ H=\\{h(x)=w^T x \\mid w\\in \\mathbb{R}^{d+1}\\}$$\n",
    "\n",
    "Notice that it is similar to the hypothesis set of the Perceptron, the only difference is that here we don't take the sign() function on the signal: $w^T x$. \n",
    "\n",
    "#### Training\n",
    "In most learning situations we have to settle for a near-optimal solution. But in Linear Regression the in-sample error function, $E_{in}(h) =  \\frac{1}{N}\\sum_{i=1}^N (h(x_i)-y_i)^2$ called __least squares__,  is \"simple\" enough to allow us to find a __closed-form solution__. Thus we can - in a sort of one step learning process - pin down a weight vector $w^*$, which define the __optimal hyperplane__ for the given dataset. The equation looks as follows:\n",
    "\n",
    "$$w^\\ast = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "Here y is a column vector of all $y_i's$ of the training data, and X is the $n * (d+1)$ matrix where each row contains the entries of a single datapoint (and first column is all ones).\n",
    "\n",
    "#### Deriving $w^\\ast$\n",
    "\n",
    "Below is the steps for deriving the equation for $w^\\ast$. We start from the error function in the form above:\n",
    "\n",
    "$$E_{in}(h) = \\frac{1}{N} \\sum_{i=1}^N (h(x_i)-y_i)^2$$\n",
    " \n",
    "As any $ h\\in H $ is completely defined by some vector of weights w, we can replace h(x):\n",
    "    \n",
    "$$E_{in}(w) = \\frac{1}{N} \\sum_{i=1}^N (w^Tx_i-y_i)^2$$\n",
    "    \n",
    "This can be rewritten to matrix/vector form. Here $Xw -y =  [x_i^Tw-y_i,..,x_n^Tw-y_n]^T$.\n",
    "    \n",
    "$$E_{in}(w) = \\frac{1}{N} ||Xw-y||^2$$\n",
    "    \n",
    "We wish to minimize $E_{in}$  with respect to w, so we find the gradient consisting of all partial derivatives and set it equal to the 0-vector.\n",
    "\n",
    "For __a 1 variable, 2 datapoint, example__ $E_{in}(w)$ looks as follows when expanded ($x_{i,j}$ is the i'th feature of the j'th datapoint):\n",
    "\n",
    "$$ E_{in}(w) = \\frac{1}{N}\\big((x_{0,1} w_0 + x_{1,1} w_1 -y_1)^2 +(x_{0,2} w_0 + x_{1,2} w_1 -y_2)^2\\big)$$\n",
    "\n",
    "So when using the chain rule to compute the gradient for the expanded $E_{in}(w)$ above we get:\n",
    "\n",
    "\n",
    "\\begin{equation} \n",
    "       \\begin{split}\n",
    "        \\nabla E_{in}(w) &=  \\frac{1}{N}{\\begin{bmatrix}\n",
    "           \\frac{\\partial E_{in}}{\\partial w_0} \\\\\n",
    "           \\frac{\\partial E_{in}}{\\partial w_1} \\\\\n",
    "         \\end{bmatrix}}\\\\\\\\\n",
    "          &= \\frac{1}{N} \\begin{bmatrix}\n",
    "           2(x_{0,1} w_0 + x_{1,1} w_1 -y_1)\\cdot (x_{0,1} \\cdot 1 + 0 - 0) + 2(x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\cdot (x_{0,2} \\cdot 1 + 0 - 0)  \\\\\n",
    "           2(x_{0,1} w_0 + x_{1,1} w_1 -y_1)\\cdot (0 + x_{1,1} \\cdot 1 - 0) + 2(x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\cdot (0 + x_{1,1} \\cdot 1 - 0)  \\\\\n",
    "         \\end{bmatrix}\\\\\\\\\n",
    "          &= \\frac{2}{N} \\begin{bmatrix}\n",
    "           (x_{0,1} w_0 + x_{1,1} w_1 -y_1)\\cdot x_{0,1} + (x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\cdot x_{0,2}  \\\\\n",
    "           (x_{0,1} w_0 + x_{1,1} w_1 -y_1)\\cdot x_{1,1} + (x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\cdot x_{1,2} \\\\\n",
    "         \\end{bmatrix}\\\\\\\\\n",
    "         &= \\frac{2}{N} \\begin{bmatrix} \n",
    "         x_{0,1} & x_{0,2}\\\\\n",
    "         x_{1,1} & x_{1,2}\\\\\n",
    "         \\end{bmatrix}\n",
    "         \\begin{bmatrix}\n",
    "           (x_{0,1} w_0 + x_{1,1} w_1 -y_1) \\\\\n",
    "           (x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\\\\n",
    "         \\end{bmatrix}\\\\\\\\\n",
    "          &= \\frac{2}{N} X^T \\begin{bmatrix}\n",
    "          (x_{0,1} w_0 + x_{1,1} w_1 -y_1) \\\\\n",
    "          (x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\\\\n",
    "         \\end{bmatrix}\\\\\\\\\n",
    "         &= \\frac{2}{N}X^T(Xw-y)\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "    \n",
    "\n",
    "    \n",
    "Now if we set it equal to the 0-vector, and rewrite a bit we get(ignoring the normalization factor):\n",
    "    \n",
    "$$X^TXw =X^Ty$$\n",
    "    \n",
    "Now we can solve for w by multiplying with the inverse of $X^TX$:\n",
    "    \n",
    "$$w^\\ast =(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "And we have a formula for THE best w for the given data.\n",
    "\n",
    "$X^\\dagger = (X^TX)^{-1}X^T$ is called the pseudo inverse of X, because if you multiply X with it, you get the identity matrix.\n",
    "\n",
    "TODO skriv mere om inverse matricer\n",
    "\n",
    "#### Linear Regression for classification\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression (classification)\n",
    "In Logistic Regression we model the the target function as a probability distribution $f(x)=p(y\\mid x)$. The output is again a real number but we choose to interpret the output as a probability of belonging in either class 0 or 1, and thus if the output is greater than 0.5 we output \"class 1\", and we output \"class 0\" otherwise. This means we can use it for classification, with the nice biproduct of knowing the probability of each class. \n",
    "\n",
    "The hypothesis set looks a follows:\n",
    "\n",
    "$$H = \\{h(x) = \\sigma(w^Tx) | w \\in \\mathbb{R}^{d+1}\\}$$\n",
    "\n",
    "Here, $\\sigma()$ refers to the sigmoid squishification function:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$ \n",
    "\n",
    "Which compress the input x into the range $]0,1[$, such that $\\sigma(x)$ is close to 1 if x is a large positive number, and close to 0 if x is a large negative number. Thus we have:\n",
    "$$\n",
    "p(y \\mid x, w) = \\left \\{\n",
    "\\begin{array}{l l}\n",
    "  \\sigma(w^\\intercal x)\n",
    "  & \\text{ if } y=1  \\\\\n",
    "  1 - \\sigma(w^\\intercal x)\n",
    "  & \\text { if } y=0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "That is, the probability that $y = 1$ given $x$ and $\\theta$\n",
    "is $\\sigma(\\theta^\\intercal x)$. Similarly, the probability that $y=0$ given $x$ and $\\theta$ is $1-\\sigma(\\theta^\\intercal x)$.\n",
    "#### Training\n",
    "\n",
    "To train a Logistic Regression model we want to maximize the following probability:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(D \\mid w)\n",
    "&= \\prod_{(x,y)\\in D}\n",
    "  p(y \\mid x,w)\\\\\n",
    "&= \\prod_{(x,y)\\in D}\n",
    "  \\sigma(w^\\intercal x )^{y}\n",
    "  (1-\\sigma(w^\\intercal x))^{1-y}\n",
    "  \\end{align}$$\n",
    "\n",
    "That is, we wish to find the w that maximize the probability of seeing D given w. We call this a __maximum likelihood estimate of w(MLE)__. The first equality comes from the assumption that all datapoints of D are sampled independently according to some unknown input distribution $P(X)$, and the second from the fact that either $\\sigma(w^\\intercal x )^{y}$ or $(1-\\sigma(w^\\intercal x))^{1-y}$ must be 1 depending on the value of y (y is 1 or 0). $p(D \\mid w)$ is called the _likelihood_ of D given w.\n",
    "\n",
    "Instead of doing a MLE on the likelihood above, we equivalently _minimize_ the __negative log likelihood(NLL)__ instead:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathrm{NLL}(D\\mid w)\n",
    "&= -\\log{p(D\\mid w)}\\\\\n",
    "&=- \\sum_{i=1}^n\n",
    "y_i \\ln(\\sigma(w^\\intercal x_i)) +\n",
    "(1-y_i) \\ln(1-\\sigma(w^\\intercal x_i))\n",
    "\\end{align}$$\n",
    "\n",
    "This is done for the following reasons: 1) Computing very small probabilities can cause numerical issues (underflow). 2) Logs are monotone increasing/decreasing. 3) Products turn into sums using logarithm rule. Notice that one of the terms in the sum is always 0.\n",
    "\n",
    "The gradient of NNL is:\n",
    "$$\n",
    "\\nabla \\mathrm{NLL}(D \\mid \\theta)\n",
    "= \\frac{\\partial \\mathrm{NLL}}{\\partial \\theta}\n",
    "= -X^\\intercal(Y-\\sigma(X\\theta))\n",
    "$$\n",
    "\n",
    "TODO vis hvordan gradienten beregnes http://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/\n",
    "\n",
    "And the final in-sample error that we use for minimization is the normalized NLL:\n",
    "\n",
    "$$E_{in}(w) = \\frac{1}{N} NLL(D\\mid w)$$\n",
    "\n",
    "This error measure does not allow an easy optimal analytical solution like in the case of Linear Regression, so we mimimize it instead using __gradient descent__. \n",
    "\n",
    "TODO GRADIENT DESCENT\n",
    "\n",
    "#### One vs all\n",
    "\n",
    "Logistic Regression classification as described above is a binary classifier. If we wish to allow more classes we can use the one vs all technique, where the problem is converted into K binary classification problems. K classifiers are then trained where all datapoints but those belonging to the current class are marked as negative samples. Predicting is then a matter of predicting on all models, and returning the class which returns the highest probability. An alternative is to use __softmax__.\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "Softmax is a generalization of logistic regression where instead of only 2 classes we now have K. This means y is no longer a column vector, but a $n x K$ matrix containing all 0's exept for one \"1\" per row, indicating which class the datapoint belongs to. \n",
    "\n",
    "Instead of the sigmoid function we now use the Softmax function, which takes a vector of length K and outputs a vector of length K. The output vector sums to one, and can be interpreted as probabilities of each of the K classes.\n",
    "\n",
    "$$\n",
    "\\textrm{softmax}(x)_j =\n",
    "\\frac{e^{x_j}}\n",
    "{\\sum_{i=1}^K e^{x_i}}\\quad\n",
    "\\textrm{ for }\\quad j = 1, \\dots, K.\n",
    "$$\n",
    "\n",
    "w now becomes a $d+1 x K$ matrix, so that we have d + 1 parameters (+1 for bias) for each of the K classes (parameters for class c is column c), meaning that for each row $w = [w_1,...,w_K]$ and then we can make our signal as a linear combination as before:\n",
    "\n",
    "$$\n",
    "p(y \\mid x,w) =\n",
    "\\textrm{softmax}(w^\\intercal x) =\n",
    " \\left \\{\n",
    "\\begin{array}{l l}\n",
    " \\textrm{softmax}(w^\\intercal x)_1 & \\text{ if } y = e_1,  \\\\\n",
    " \\vdots & \\\\\n",
    " \\textrm{softmax}(w^\\intercal x)_K & \\text { if } y = e_K.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Non-linear transforms\n",
    "\n",
    "The models above are called \"linear\", because the of the linearity of the signal:\n",
    "\n",
    "$$w^T x = \\sum_{i=0}^d w_i x_i$$\n",
    "\n",
    "The above sum is linear in both $w_i$ and $x_i$, but since the $x_i$'s are just constants in this situation, modifying them does not break the linearity in $w_i$. The linearity in the variables $w_i$ is what makes these models _linear_. We can exploit this to transform the data nonlinearly. A classic example is shown below:\n",
    "\n",
    "![title](imgs/nonlintransforms.png)\n",
    "\n",
    "The data is transformed from X space into Z space by the transformation $\\phi$ which just computes the squares. In Z space, we can see that the data becomes linearly separable. So we can use this \"new\" training data and train a linear model, and when we get a new datapoint we transform it similarly before prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
