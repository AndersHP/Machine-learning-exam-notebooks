{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6) Hidden Markov Models - Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden Markov Models\n",
    "* 3 Problems for HMM's\n",
    "    * Compute P(X|M)\n",
    "    * __Decoding__\n",
    "    * __Training__  \n",
    "* Decoding\n",
    "    * __Viterbi__ \n",
    "    * __Posterior__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models\n",
    "\n",
    "In many other machine learning situations we have the assumption of __independent and identically distributed__ data. This is not always a reasonable assumption. For example with __sequential data__ such as weather observations (rainy, cloudy, sunny, etc.), the probability of seeing rain one day, is affected by which type of weather was observed the day before (and possibly further back). This leads us to consider __markov models__ in which the probability of an observation is independent of all but the most recent observations. If the probability is affected by only the previous observation, then we call it a first-order markov model, and in general we can have __i'th-order markov models__ where:\n",
    "\n",
    "$$ p(x_N \\rvert x_1,...,x_{N-1}) = p(x_n \\rvert x_{N-i},..., x_{N-1})$$\n",
    "\n",
    "If the observations are affected by latent (or hidden) discrete variables, and form a markov chain, then we have a __hidden markov model (hmm)__. For the weather observations, is not actually the outcome of the previous days, but rather its affected by low/high pressure areas.\n",
    "\n",
    "A k-state hmm can be represented by a 3-tuple of matrices $(\\pi,A,\\theta)$:\n",
    "\n",
    "-  $\\pi: k x 1$ matrix of initial state probabilities\n",
    "-  $A: k\\ x\\ k$ matrix of transition probabilities.\n",
    "-  $\\theta: k\\ x\\ |\\Sigma|$ matrix of emission probabilities\n",
    "\n",
    "Where $\\Sigma$ is the \"emission alphabet\". For the weather example it contains \"sun\", \"rain\", \"cloudy\" etc.  \n",
    "\n",
    "__A hmm generates a sequence of observables by jumping from state to state, according to A, each time emitting an observable according to $\\theta$.__\n",
    "\n",
    "An example observations sequence X, with corresponding underlying state sequence Z, can be seen below for the weather example:\n",
    "\n",
    "<img src=\"imgs\\hmm.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Problems for hmm's\n",
    "\n",
    "For hmm's there are 3 basic problems which must be solved for them to be useful.\n",
    "\n",
    "#### 1) Compute P(X|M) \n",
    "\n",
    "To compute $P(X\\rvert M)$, that is, the probability of seeing some sequence of observables given a hmm M. To do this we can argue that the following holds:\n",
    "\n",
    "$$P(X\\rvert M) = \\sum_{z\\in Z} P(X,Z\\rvert M)$$\n",
    "\n",
    "The probability of seeing an observation sequence is the sum of all joint probabilities with different underlying state sequences (different state sequences can produce the same sequence of observables). There are $k^N$ different state sequences possible - where k is the number of states in the hmm - so computing this directly is infeasible. Instead it can be calculated in time $O(K^2N)$ time as a bi-product of posterior decoding, by summing the last column of the $\\alpha$-table.\n",
    "\n",
    "\n",
    "##### Joint probability P(X,Z | M)\n",
    "\n",
    "The formula for computing the joint probability looks as follows:\n",
    "\n",
    "$$ P(X,Z \\rvert M) = P(z_1 \\rvert \\pi) \\bigg[\\prod_{n=2}^N P(z_n \\rvert z_{n-1}, A)\\bigg] \\prod_{n=2}^N P(x_n \\rvert z_n, \\theta)$$\n",
    "\n",
    "-  $P(z_1 \\rvert \\pi)$  is the probability of starting in state $z_1$\n",
    "-  $\\prod_{n=2}^N P(z_n \\rvert z_{n-1}, A)$ is the probability of going though the state sequence $z_2$ to $z_N$.\n",
    "-  $\\prod_{n=1}^N P(x_n \\rvert z_n, \\theta)$ is the probability of observing X for the state sequence Z.\n",
    "\n",
    "N is often a very large number, so the joint probability can be come _very_ small. To avoid numerical underflow one can compute $log(P(X,Z \\rvert M)$ instead.\n",
    "\n",
    "#### 2) Decoding\n",
    "\n",
    "The second basic problem for hmm's is __decoding.__ Here we are interested in uncovering the most hidden parts of the model. There are two interpretations for this. One is finding the most likely hidden state sequence which produced a given observation sequence (__Viterbi__). Another is finding the individually most likely state to be in at a given point in the observation sequence (__Posterior__).\n",
    "\n",
    "##### Viterbi decoding\n",
    "\n",
    "In viterbi decoding we wish to find the following:\n",
    "\n",
    "$$Z^\\ast = arg \\underset{Z}{\\operatorname{max}} P(X,Z \\rvert M) $$\n",
    "\n",
    "That is, the state sequence Z that maximize the joint probability $P(X,Z \\rvert M)$. We do this in 3 steps:\n",
    "\n",
    "-  Compute the $\\omega$-table.\n",
    "-  Pick the row with the largest value in the last column.\n",
    "-  Backtrack to obtain optimal path.\n",
    "\n",
    "##### Posterior  decoding\n",
    "\n",
    "In posterior decoding we wish to find the following:\n",
    "\n",
    "$$z^\\ast_n = arg \\underset{z_n}{\\operatorname{max}} P(z_n  \\rvert x_1,...x_N) $$\n",
    "\n",
    "That is, the most likely state to be in at the n'th step given an observation sequence. We do this in x steps:\n",
    "\n",
    "1) Compute the $\\alpha$-table and the $\\beta$-table.\n",
    "\n",
    "2) TODO\n",
    "\n",
    "\n",
    "#### 3) Training\n",
    "\n",
    "Training is the third basic problem for hmm's. It is the problem of selecting model parameters $(\\pi,A,\\theta)$, to reflect given (X,Z) pair's or just a set of X's.\n",
    "\n",
    "##### Training by counting\n",
    "\n",
    "If we are given several sequences of observations and corresponding latent states - how do we set model parameters to make the given (X,Z)'s most likely to occur? The parameters should reflect what we have seen.\n",
    "\n",
    "In \"training by counting\" we count the relevant occurrences and adjust the model parameters to reflect these. This yields a __maximum likelihood estimation__ of $M = (\\pi, A, \\theta)$ by maximizing the likelihood:\n",
    "\n",
    "$$P(X\\rvert M) = \\sum_{z\\in Z} P(X,Z\\rvert M)$$\n",
    "\n",
    "##### Viterbi training\n",
    "\n",
    "If we only have the X's to work with - that is, the Z's are unknown - we can use Viterbi training. It involves 4 steps:\n",
    "\n",
    "1) Decide on initial parameters $M_0 = (\\pi_0, A_0, \\theta_0)$.\n",
    "\n",
    "2) Find the most likely sequence of states $Z^\\ast$ explaining X using Viterbi decoding and the current parameters $M_i$.\n",
    "\n",
    "3) Update parameters to $M_{i+1}$ by “counting” (with pseudo counts) according to $(X,Z^\\ast)$.\n",
    "\n",
    "4) Repeat 2-3 until $P(X,Z^\\ast | M_i)$ is satisfactory (or the Viterbi sequence of states does not change).\n",
    "\n",
    "This yields a local maximum of: \n",
    "\n",
    "$$VIT_X(M) = max_Z P(X,Z \\rvert M) $$\n",
    "\n",
    "Which is not a MLE, but it works ok.\n",
    "\n",
    "##### Expectation Maximization\n",
    "\n",
    "If we only have the X's to work with we can still do a MLE using Expectation Maximization:\n",
    "\n",
    "Init:   Pick “suitable” parameters (transition and emission probabilities).\n",
    "\n",
    "E-step: 1) Run the forward- and backward-algorithms with the current choice of parameters (to get the params of Q-func).\n",
    "\n",
    "Stop ?: 2) Compute the likelihood $P(X \\rvert M)$, if sufficient (or another stopping criteria is met) then stop.\n",
    "\n",
    "M-step: 3) Compute new parameters using the values stored by the forward- and backward-algorithms. Repeat 1-3.\n",
    "\n",
    "Each iteration of the above states makes the likelihood $P(X \\rvert M)$ converge to local max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "\n",
    "#### Viterbi decoding \n",
    "\n",
    "As mentioned earlier, Viterbi decoding is about finding the following state sequence:\n",
    "\n",
    "$$Z^\\ast = arg \\underset{Z}{\\operatorname{max}} P(X,Z \\rvert M) $$\n",
    "\n",
    "That is, the state sequence Z that maximize the joint probability $P(X,Z \\rvert M)$. To find $Z^\\ast$ we first compute the $(k\\ x\\ N)$-sized $\\omega$-table where the entry $\\omega(z_n)$ is the probability of the most likely sequence of states $z_1,z_2,...,z_n$, ending in $z_n$, that generated the observations $x_1, x_2,...,x_n$. i.e.:\n",
    "\n",
    "$$\\boxed{\\omega(z_n) = \\underset{z_1,...,z_{n-1}}{\\operatorname{max}} P(x_1,...,x_n,z_1,...,z_n)} $$\n",
    "\n",
    "If we look at the following formula, we can see how we came up with the definition for $\\omega(z_n)$:\n",
    "\n",
    "\\begin{equation} \n",
    "       \\begin{split}\n",
    "          P(X,Z^\\ast) &=  \\underset{Z}{\\operatorname{max}} P(X,Z)\\\\\n",
    "          &= \\underset{z_1,...z_N}{\\operatorname{max}} P(x_1,...,x_N,z_1,...,z_N)\\\\\\\\\n",
    "          &= \\underset{z_N}{\\operatorname{max}} \\boxed{\\underset{z_1,...z_{N-1}}{\\operatorname{max}}  P(x_1,...,x_N,z_1,...,z_N)}\\\\\n",
    "         &= \\underset{z_N}{\\operatorname{max}} \\boxed{\\omega(z_N)}\\\\\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "\n",
    "##### Computing omega table\n",
    "\n",
    "The $\\omega$-table will be computed recursively column by column left to right. The first column will be filled using the base step described below. In the recursive step we take the previous column into consideration.\n",
    "\n",
    "__Base:__ $\\omega(z_1): P(x_1, z_1) = P(z_1)P(x_1 \\rvert z_1)$\n",
    "\n",
    "__Recu:__ $\\omega(z_n):  \\underset{z_{n-1}}{\\operatorname{max}} \\big[ \\omega(z_{n-1}) P(z_n \\rvert z_{n-1}) \\big] P(x_n \\rvert z_n)$\n",
    "\n",
    "\n",
    "After computing the table we can find the most likely state sequence $Z^\\ast$ which was what we wanted in the first place. We do this by __backtracking__ through the table: \n",
    "\n",
    "-  First let $z_N$ be the argmax of the last column\n",
    "-  Let $Z_{N-1}$ be the element of column N-1 that was used to compute the probability of the argmax in column N\n",
    "above.\n",
    "-  Continue as above\n",
    "\n",
    "\n",
    "#### Posterior decoding\n",
    "- Given X of length n, whats most likely state to be in at time n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
