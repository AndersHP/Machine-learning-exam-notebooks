{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6) Hidden Markov Models - Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden Markov Models\n",
    "* 3 Problems for HMM's\n",
    "    * Compute P(X|M)\n",
    "    * __Decoding__\n",
    "    * __Training__  \n",
    "* Decoding\n",
    "    * __Viterbi__ \n",
    "    * __Posterior__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models\n",
    "\n",
    "In many other machine learning situations we have the assumption of __independent and identically distributed__ data. This is not always a reasonable assumption. For example with __sequential data__ such as weather observations (rainy, cloudy, sunny, etc.), the probability of seeing rain one day, is affected by which type of weather was observed the day before (and possibly further back). This leads us to consider __markov models__ in which the probability of an observation is independent of all but the most recent observations. If the probability is affected by only the previous observation, then we call it a first-order markov model, and in general we can have __i'th-order markov models__ where:\n",
    "\n",
    "$$ p(x_N \\rvert x_1,...,x_{N-1}) = p(x_n \\rvert x_{N-i},..., x_{N-1})$$\n",
    "\n",
    "If the observations are affected by latent (or hidden) discrete variables, and form a markov chain, then we have a __hidden markov model (hmm)__. For the weather observations, is not actually the outcome of the previous days, but rather its affected by low/high pressure areas.\n",
    "\n",
    "A k-state hmm can be represented by a 3-tuple of matrices $(\\pi,A,\\theta)$:\n",
    "\n",
    "-  $\\pi: k x 1$ matrix of initial state probabilities\n",
    "-  $A: k\\ x\\ k$ matrix of transition probabilities.\n",
    "-  $\\theta: k\\ x\\ |\\Sigma|$ matrix of emission probabilities\n",
    "\n",
    "Where $\\Sigma$ is the \"emission alphabet\". For the weather example it contains \"sun\", \"rain\", \"cloudy\" etc.  \n",
    "\n",
    "__A hmm generates a sequence of observables by jumping from state to state, according to A, each time emitting an observable according to $\\theta$.__\n",
    "\n",
    "An example observations sequence X, with corresponding underlying state sequence Z, can be seen below for the weather example:\n",
    "\n",
    "<img src=\"imgs\\hmm.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Problems for hmm's\n",
    "\n",
    "For hmm's there are 3 basic problems which must be solved for them to be useful.\n",
    "\n",
    "#### 1) Compute P(X|M) \n",
    "\n",
    "To compute $P(X\\rvert M)$, that is, the probability of seeing some sequence of observables given a hmm M. To do this we can argue that the following holds:\n",
    "\n",
    "$$P(X\\rvert M) = \\sum_{z\\in Z} P(X,Z\\rvert M)$$\n",
    "\n",
    "The probability of seeing an observation sequence is the sum of all joint probabilities with different underlying state sequences (different state sequences can produce the same sequence of observables). There are $k^N$ different state sequences possible - where k is the number of states in the hmm - so computing this directly is infeasible. Instead it can be calculated in time $O(K^2N)$ time as a bi-product of posterior decoding, by summing the last column of the $\\alpha$-table.\n",
    "\n",
    "\n",
    "##### Joint probability P(X,Z | M)\n",
    "\n",
    "The formula for computing the joint probability looks as follows:\n",
    "\n",
    "$$ P(X,Z \\rvert M) = P(z_1 \\rvert \\pi) \\bigg[\\prod_{n=2}^N P(z_n \\rvert z_{n-1}, A)\\bigg] \\prod_{n=2}^N P(x_n \\rvert z_n, \\theta)$$\n",
    "\n",
    "-  $P(z_1 \\rvert \\pi)$  is the probability of starting in state $z_1$\n",
    "-  $\\prod_{n=2}^N P(z_n \\rvert z_{n-1}, A)$ is the probability of going though the state sequence $z_2$ to $z_N$.\n",
    "-  $\\prod_{n=1}^N P(x_n \\rvert z_n, \\theta)$ is the probability of observing X for the state sequence Z.\n",
    "\n",
    "N is often a very large number, so the joint probability can be come _very_ small. To avoid numerical underflow one can compute $log(P(X,Z \\rvert M)$ instead.\n",
    "\n",
    "#### 2) Decoding\n",
    "\n",
    "The second basic problem for hmm's is __decoding.__ Here we are interested in uncovering the hidden parts of the model. There are two interpretations for this. One is finding the most likely hidden state sequence which produced a given observation sequence (__Viterbi__). Another is finding the individually most likely state to be in at a given point in the observation sequence (__Posterior__).\n",
    "\n",
    "##### Viterbi decoding\n",
    "\n",
    "In viterbi decoding we wish to find the following:\n",
    "\n",
    "$$Z^\\ast = arg \\underset{Z}{\\operatorname{max}} P(X,Z \\rvert M) $$\n",
    "\n",
    "That is, the state sequence Z that maximize the joint probability $P(X,Z \\rvert M)$. We do this in 3 steps:\n",
    "\n",
    "-  Compute the $\\omega$-table.\n",
    "-  Pick the row with the largest value in the last column.\n",
    "-  Backtrack to obtain optimal path.\n",
    "\n",
    "##### Posterior  decoding\n",
    "\n",
    "In posterior decoding we wish to find the following:\n",
    "\n",
    "$$z^\\ast_n = arg \\underset{z_n}{\\operatorname{max}} P(z_n  \\rvert x_1,...x_N) $$\n",
    "\n",
    "That is, the most likely state to be in at the n'th step given an observation sequence. We do this in 3 steps:\n",
    "\n",
    "1) Compute the $\\alpha$-table.\n",
    "\n",
    "2) Compute the $\\beta$-table.\n",
    "\n",
    "2) Find the individually best states as: $z^\\ast_n = arg \\underset{z_n}{\\operatorname{max}}\\frac{\\alpha (z_n)\\beta (z_n)}{P(X)} = arg \\underset{z_n}{\\operatorname{max}}\\frac{\\alpha (z_n)\\beta (z_n)}{\\sum_{z_N} \\alpha (z_N)}$\n",
    "\n",
    "\n",
    "#### 3) Training\n",
    "\n",
    "Training is the third basic problem for hmm's. It is the problem of selecting model parameters $(\\pi,A,\\theta)$, to reflect given (X,Z) pair's or just a set of X's.\n",
    "\n",
    "##### Training by counting\n",
    "\n",
    "If we are given several sequences of observations and corresponding latent states - how do we set model parameters to make the given (X,Z)'s most likely to occur? The parameters should reflect what we have seen.\n",
    "\n",
    "In \"training by counting\" we count the relevant occurrences and adjust the model parameters to reflect these. This yields a __maximum likelihood estimation__ of $M = (\\pi, A, \\theta)$ by maximizing the likelihood:\n",
    "\n",
    "$$P(X\\rvert M) = \\sum_{z\\in Z} P(X,Z\\rvert M)$$\n",
    "\n",
    "##### Viterbi training\n",
    "\n",
    "If we only have the X's to work with - that is, the Z's are unknown - we can use Viterbi training. It involves 4 steps:\n",
    "\n",
    "1) Decide on initial parameters $M_0 = (\\pi_0, A_0, \\theta_0)$.\n",
    "\n",
    "2) Find the most likely sequence of states $Z^\\ast$ explaining X using Viterbi decoding and the current parameters $M_i$.\n",
    "\n",
    "3) Update parameters to $M_{i+1}$ by “counting” (with pseudo counts) according to $(X,Z^\\ast)$.\n",
    "\n",
    "4) Repeat 2-3 until $P(X,Z^\\ast | M_i)$ is satisfactory (or the Viterbi sequence of states does not change).\n",
    "\n",
    "This yields a local maximum of: \n",
    "\n",
    "$$VIT_X(M) = max_Z P(X,Z \\rvert M) $$\n",
    "\n",
    "Which is not a MLE, but it works ok.\n",
    "\n",
    "##### Expectation Maximization\n",
    "\n",
    "If we only have the X's to work with we can still do a MLE using Expectation Maximization:\n",
    "\n",
    "Init:   Pick “suitable” parameters (transition and emission probabilities).\n",
    "\n",
    "E-step: 1) Run the forward- and backward-algorithms with the current choice of parameters (to get the params of Q-func).\n",
    "\n",
    "Stop ?: 2) Compute the likelihood $P(X \\rvert M)$, if sufficient (or another stopping criteria is met) then stop.\n",
    "\n",
    "M-step: 3) Compute new parameters using the values stored by the forward- and backward-algorithms. Repeat 1-3.\n",
    "\n",
    "Each iteration of the above states makes the likelihood $P(X \\rvert M)$ converge to local max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding\n",
    "\n",
    "#### Viterbi decoding \n",
    "\n",
    "As mentioned earlier, Viterbi decoding is about finding the following state sequence:\n",
    "\n",
    "$$Z^\\ast = arg \\underset{Z}{\\operatorname{max}} P(X,Z \\rvert M) $$\n",
    "\n",
    "That is, the state sequence Z that maximize the joint probability $P(X,Z \\rvert M)$. To find $Z^\\ast$ we first compute the $(k\\ x\\ N)$-sized $\\omega$-table where the entry $\\omega(z_n)$ is the probability of the most likely sequence of states $z_1,z_2,...,z_n$, ending in $z_n$, that generated the observations $x_1, x_2,...,x_n$. i.e.:\n",
    "\n",
    "$$\\boxed{\\omega(z_n) = \\underset{z_1,...,z_{n-1}}{\\operatorname{max}} P(x_1,...,x_n,z_1,...,z_n)} $$\n",
    "\n",
    "If we look at the following formula, we can see how we came up with the definition for $\\omega(z_n)$:\n",
    "\n",
    "\\begin{equation} \n",
    "       \\begin{split}\n",
    "          P(X,Z^\\ast) &=  \\underset{Z}{\\operatorname{max}} P(X,Z)\\\\\n",
    "          &= \\underset{z_1,...z_N}{\\operatorname{max}} P(x_1,...,x_N,z_1,...,z_N)\\\\\\\\\n",
    "          &= \\underset{z_N}{\\operatorname{max}} \\boxed{\\underset{z_1,...z_{N-1}}{\\operatorname{max}}  P(x_1,...,x_N,z_1,...,z_N)}\\\\\n",
    "         &= \\underset{z_N}{\\operatorname{max}} \\boxed{\\omega(z_N)}\\\\\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "\n",
    "##### Computing omega table\n",
    "\n",
    "The $\\omega$-table will be computed recursively, column by column, from left to right. The first column will be filled using the base step described below. In the recursive step we take the previous column into consideration.\n",
    "\n",
    "__Base:__ $\\omega(z_1)= P(x_1, z_1) = P(z_1)P(x_1 \\rvert z_1)$\n",
    "\n",
    "__Recu:__ $\\omega(z_n)=  \\underset{z_{n-1}}{\\operatorname{max}} \\big[ \\omega(z_{n-1}) P(z_n \\rvert z_{n-1}) \\big] P(x_n \\rvert z_n)$\n",
    "\n",
    "\n",
    "After computing the table we can find the most likely state sequence $Z^\\ast$ which was what we wanted in the first place. We do this by __backtracking__ through the table: \n",
    "\n",
    "-  First let $z_N^\\ast$ be the argmax of the last column\n",
    "-  Let $z_{N-1}^\\ast$ be the element of column N-1 that was used to compute the probability of the argmax in column N\n",
    "above.\n",
    "-  Continue as above\n",
    "\n",
    "\n",
    "## Bishops forklaring af  viterbi decoding ( læs, forstå og slet)\n",
    "\n",
    "Intuitively, we can understand the Viterbi algorithm as follows. Naively, we\n",
    "could consider explicitly all of the exponentially many paths through the lattice,\n",
    "evaluate the probability for each, and then select the path having the highest probability.\n",
    "However, we notice that we can make a dramatic saving in computational cost\n",
    "as follows. Suppose that for each path we evaluate its probability by summing up\n",
    "products of transition and emission probabilities as we work our way forward along\n",
    "each path through the lattice. Consider a particular time step n and a particular state\n",
    "k at that time step. There will be many possible paths converging on the corresponding\n",
    "node in the lattice diagram. However, we need only retain that particular path\n",
    "that so far has the highest probability. Because there are K states at time step n, we\n",
    "need to keep track of K such paths. At time step n + 1, there will be $K^2$ possible\n",
    "paths to consider, comprising K possible paths leading out of each of the K current\n",
    "states, but again we need only retain K of these corresponding to the best path for\n",
    "each state at time n+1. When we reach the final time step N we will discover which\n",
    "state corresponds to the overall most probable path. Because there is a unique path\n",
    "coming into that state we can trace the path back to step N − 1 to see what state it\n",
    "occupied at that time, and so on back through the lattice to the state n = 1.\n",
    "\n",
    "\n",
    "#### Posterior decoding\n",
    "\n",
    "In Posterior decoding we seek to find the _individually_ most likely states given some observation sequence X. i.e.:\n",
    "\n",
    "$$z^\\ast_n = arg \\underset{z_n}{\\operatorname{max}} P(z_n  \\rvert x_1,...x_N) $$\n",
    "\n",
    "The formula for any such conditional probability is:\n",
    "\n",
    "\\begin{equation} \n",
    "       \\begin{split}\n",
    "          P(z_n \\rvert x_1,...,x_N) &= \\frac{P(z_n, x_1,...,x_N)}{P(x_1,...,x_N)}\\\\\n",
    "          &= \\frac{P(x_1,...,x_n, z_n)P(x_{n+1},...,x_N \\rvert z_n)}{P(x_1,...,x_N)}\\\\\n",
    "          &= \\frac{\\alpha (z_n)\\beta (z_n)}{P(X)}\\\\\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "\n",
    "Where we use the fact that $P(z_n, x_1,...,x_N) = P(x_1,...,x_n, z_n)P(x_{n+1},...,x_N \\rvert z_n)$... TODO why is that valid???\n",
    "\n",
    "Furthermore we have defined the forward/backward variables as:\n",
    "\n",
    "$$\\alpha (z_n) =  P(x_1,...,x_n, z_n)\\\\\n",
    "  \\beta (z_n) =  P(x_{n+1},...,x_N \\rvert z_n)\n",
    "$$\n",
    "\n",
    "__And we can now express the optimal states in terms of these variables:__\n",
    "\n",
    "$$z^\\ast_n = arg \\underset{z_n}{\\operatorname{max}}\\frac{\\alpha (z_n)\\beta (z_n)}{P(X)} $$\n",
    "\n",
    "As mentioned earlier we can compute the denominator $P(X)$ by summing the last column of the $\\alpha$-table. \n",
    "\n",
    "\n",
    "##### Computing alpha table\n",
    "\n",
    "The $\\alpha$-table is computed very similar to the $\\omega$-table used in Viterbi decoding. The base step is the same, and the recursive step differs only in that here we sum over the elements in the previous column, whereas in the computation of the $\\omega$ table, we use the max operator instead. \n",
    "\n",
    "__Base:__ $\\alpha(z_1) = P(x_1, z_1) = P(z_1)P(x_1 \\rvert z_1)$\n",
    "\n",
    "__Recu:__ $\\alpha(z_n) = \\big[ \\sum_{z_{n-1}} \\alpha(z_{n-1})P(z_n \\rvert z_{n-1}) \\big] P(x_n \\rvert z_n) $\n",
    "\n",
    "__Explanation of the recursive step:__\n",
    "\n",
    "Since $\\alpha(z_{n-1})$ is the joint probability of observing $x_1,...x_{n-1}$ and being in state $z_{n-1}$ at time n-1, $\\alpha(z_{n-1})P(z_n \\rvert z_{n-1})$ is then the probability of the joint event that $x_1,...x_{n-1}$ are observed and state $z_n$ is reached at time n via state $z_{n-1}$. So when summing these products, we have accounted for the different ways we could reach $z_n$ while seeing $x_1,...x_{n-1}$. After summing we multiply with  $P(x_n \\rvert z_n)$ to account for seeing the entire sequence $x_1,...x_{n}$.\n",
    "\n",
    "![title](imgs\\alpha.PNG)\n",
    "\n",
    "##### Computing beta table\n",
    "\n",
    "The beta table is computed right to left.\n",
    "\n",
    "__Base:__ $\\beta(z_N) = 1$\n",
    "\n",
    "__Recu:__ $\\beta(z_n) = \\sum_{z_{n+1}} \\beta(z_{n+1}) P(x_{n+1} \\rvert z_{n+1}) P(z_{n+1} \\rvert z_n)  $\n",
    "\n",
    "__Explanation of the recursive step:__\n",
    "\n",
    "In order to have been in state $z_n$  at time n and to account for later observations $x_{n+1},...x_{N}$, we must consider all possible states $z_{n+1}$ at time n+1, accounting for the transition from $z_n$ to the $z_{n+1}$'s, as well as possible observations $x_{n+1}$, and finally the remaining partial observation sequence from state $z_{n+1}$\n",
    "\n",
    "\n",
    "![title](imgs\\beta.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
