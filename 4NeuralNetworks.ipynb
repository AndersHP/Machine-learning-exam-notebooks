{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supervised learning (briefly)\n",
    "* Feed forward neural nets\n",
    "* Training (backprop)\n",
    "* Convolutional neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Supervised learning\n",
    "\n",
    "In the Supervised Learning setting we are given a dataset $D = \\{(x_1,y_1),...,(x_n,y_n)\\}$ to learn from. The way we \"learn\" is by using D to search a hypothesis space H for some \"best\" hypothesis $g\\in H$. By \"best\", we mean that we want g to be as close to the __ unknown target function f__ as possible, i.e.: \n",
    "\n",
    "$$g \\approx f$$\n",
    "\n",
    "We most often find g by minimizing an in-sample error function $E_{in}(h)$. The process of finding g is what we call \"training\" and when we have found it, we can use it to predict on new inputs.\n",
    "\n",
    "Within supervised learning we distinguish between __two subcategories__. If the output y is a real number we call it __regression__. If the output y is a member of a discrete set - e.g. $y \\in \\{red,blue,green\\}$ - then we call it __classification__.\n",
    "\n",
    "In Supervised Learning we try to minimize $E_{in}(g)$, but what we _really_ care about in the end is having a low out-of-sample error $E_{out}(g)$, that is, in the end we only care about how g performs on _new_ data. Since we cannot measure $E_{out}(g)$ during training, we hope that we can generalize such that $E_{in}(g)$ - which we _can_ measure - is close to $E_{out}(g)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward neural networks\n",
    "\n",
    "PROJECT: 1 hidden layer of size 128. 784 input nodes. 10 output nodes.\n",
    "\n",
    "We shall focus on the feed forward type of neural nets. An example of such a network is shown below.\n",
    "\n",
    "![title](imgs/ffnn.png)\n",
    "\n",
    "It consists of a number of layers, each containing a number of neurons. The first layer is called the input layer, and the last is called the output layer. The layers in between are called __hidden layers__. The layers are connected and computation flows from the input layer towards the output layer. There are __weights__ associated with each of these connections. For each layer there is also a __bias__ each neuron. Some neural nets allow cycles such that there are arrows from one layer back to an earlier layer, but for a feed forward neural net the graph __must be acyclic__. The output layer and the hidden layers have a __activation function (or non-linearity)__ associated with it. Common choices are __sigmoid, tangh or ReLU__. If there is more than one or two hidden layers, then it is a __deep network (deep learning)__. If the task is __regression__, then the output layer may consist of a single neuron which may or may not have an activation function. If the task is __classification__, then the network may have k output neurons - one for each class. \n",
    "\n",
    "Neural networks are __universal representors__ meaning that for any continous function f, there is a neural net that can approximate f to _any_  degree of accuracy (might be a huge net). This tells us that the hypothesis set for neural nets is complex, and gets more complex as the size of the net grows. We could keep the complexity in check by keeping the net small, __but often it is better use increased approximation power that comes with a big net, and then make sure we dont fit too much using regularization__.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dropout? early stopping?? TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural net\n",
    "\n",
    "The cost function is least squares, and for neural nets which often have thousands of parameters, this function is not generally convex. Nonetheless, we can still use gradient descent to converge to some local minimum.\n",
    "\n",
    "\n",
    "To compute all partial derivatives (with respect to all w's and b's) efficiently, the __backpropagation__ technique is used.\n",
    "\n",
    "\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "Average squared error of all datapoints:\n",
    "\n",
    "<img src=\"imgs/costnn.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"imgs/backp.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "GRADIENT BACKProp: Average \"nudge\" of ALL trainingdata. Minibatch stochastic grad: average of small subset\n",
    "\n",
    "<img src=\"imgs/gradientbackp.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "#### Negative gradient of cost function\n",
    "\n",
    "__ Negative gradient of cost function tells us how to change all weights and biases to most effectively reduce the cost fonction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep convolutional neural networks\n",
    "\n",
    "IN PROJECT: ADDED 2 (convolution -> relu -> pooling) layers to standard 1 layer hidden network made in prev exercise\n",
    "\n",
    "Deep convolutional nets perform well for tasks like image recognition. If we want to learn to recognize hand-written digits from 0 to 9, then we could sit down and try to create a bunch of low level features like edges, curves or lines. If we get enough of these such that they can \"describe\" all digits from 0 to 9, then we can use them as input to a learning algorithm. But if the task is recognizing dogs or cats, then this approach is not feasible. Instead we can augment a \"standard\" neural net with __convolutional layers__, which will take care of learning these features for us. The architecture is shown below:\n",
    "\n",
    "<img src=\"imgs/cnn.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "We can choose to chain the convolutional layers, to learn increasingly higher level features. Each convolutional layer could (but does not have to) consist of a triple __(actual conv. layer, non-linearity layer, pooling layer)__, and the output of one such triple could feed into a new one. Such a triple is shown below:\n",
    "\n",
    "\n",
    "<img src=\"imgs/cnn2.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "The number of feature extractors for a given convolutional layer (#cv_features in picture), and the choice of pooling parameters determine the dimensionality of the output into the next layer. \n",
    "\n",
    "\n",
    "### Nice\n",
    "\n",
    "Convolution has the nice property of being translational invariant. Intuitively, this means that each convolution filter represents a feature of interest (e.g whiskers, fur), and the CNN algorithm learns which features comprise the resulting reference (i.e. cat). The output signal strength is not dependent on where the features are located, but simply whether the features are present. Hence, a cat could be sitting in different positions, and the CNN algorithm would still be able to recognize it.\n",
    "\n",
    "https://www.quora.com/What-is-a-convolutional-neural-network\n",
    "\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "When using cnn's we dont have to specify which features goes into the network: the networks learn the relevant features on its own, and we only have to specify the __hyperparameters__ such as filter size, pooling patch size, different layer size/channels and the size of the padding if we choose to use that. \n",
    "\n",
    "#### Padding\n",
    "\n",
    "Doing convolution reduce the dimensionality (x,y) a bit every time. If we want to avoid that, we can use a padding around the input, which has weight 0. \n",
    "\n",
    "#### Pooling /subsampling\n",
    "\n",
    "Pooling reduce the problem size. Input is output of conv layer. Pooling patch, pick largest value of each patch.\n",
    "\n",
    "QUORA:  https://www.quora.com/What-is-a-convolutional-neural-network\n",
    "\n",
    "Inputs from the convolution layer can be “smoothened” to reduce the sensitivity of the filters to noise and variations. This smoothing process is called subsampling, and can be achieved by taking averages or taking the maximum over a sample of the signal. Examples of subsampling methods (for image signals) include reducing the size of the image, or reducing the color contrast across red, green, blue (RGB) channels.\n",
    "\n",
    "#### Vanishing gradient\n",
    "\n",
    "Vanishing gradient: IF very deep network, gradient might vanish if gradients are < 1. Means last layer gets 0 gradient, and they train very slow. Sigmod derivative sigmoid' (sigmoid) * (1-sigmoid) can max be 0.25. Relu faster better. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
