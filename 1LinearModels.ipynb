{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supervised learning (briefly)\n",
    "* Perceptron (briefly)\n",
    "* Linear Regression\n",
    "    * Hypothesis set\n",
    "    * Error function\n",
    "    * Formula for $w^\\ast$\n",
    "* Logistic Regression (classificaion)\n",
    "    * Hypothesis set\n",
    "    * Sigmoid function\n",
    "    * Error function\n",
    "    * Gradient descent\n",
    "    * Softmax \n",
    "* Non-linear transforms (briefly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "In the Supervised Learning setting we are given a dataset $D = \\{(x_1,y_1),...,(x_n,y_n)\\}$ to learn from. The way we \"learn\" is by using D to search a hypothesis space H for some \"best\" hypothesis $g\\in H$. By \"best\", we mean that we want g to be as close to the __ unknown target function f__ as possible, i.e.: \n",
    "\n",
    "$$ g \\approx f $$\n",
    "\n",
    "We most often find g by minimizing an in-sample error function $E_{in}(h)$. The process of finding g is what we call \"training\" and when we have found it, we it to predict on new inputs.\n",
    "\n",
    "Within supervised learning we distinguish between __two subcategories__. If the output y is a real number we call it __regression__. If the output y is a member of a discrete set - e.g. $y \\in \\{red,blue,green\\}$ - then we call it __classification__.\n",
    "\n",
    "In Supervised Learning we try to minimize $E_{in}(g)$, but what we _really_ care about in the end is having a low out-of-sample error $E_{out}(g)$, that is, in the end we only care about how g performs on _new_ data. Since we cannot measure $E_{out}$ during training, we must try to make g __generalize__ well, such that it performs well on new unseen data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "The perceptron is the simplest of the linear models. It is a binary __classification__ model ({-1,+1}), and the hypothesis set looks as follows:\n",
    "\n",
    "$$ H=\\{h(x)=\\text{sign}(w^T x) \\mid w\\in \\mathbb{R}^{d+1}\\}$$\n",
    "\n",
    "The __Perceptron Learning Algorithm(PLA)__ search for a hyperplane which separate all datapoints correctly. While a misclassified point x exists, the algorithm will update the hypothesis, effectively moving the hyperplane in the direction of correctly classifying x. The algorithm never terminate if the given data is not linearly separable, but if the data is linearly separable, it is guaranteed to pick a $g\\in H$ with $E_{in} = 0$. \n",
    "\n",
    "The error function we wish to minimize is:\n",
    "\n",
    "$$E_{in}(h)=\\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{h(x_i)\\neq y_i} $$\n",
    "\n",
    "Where $\\mathbb{1}_{h(x_i)\\neq y_i} = 1$ if h misclassifies $x_i$, and 0 if not.  \n",
    "\n",
    "With a simple augmentation, the PLA can be used on non linearly separable datasets. The idea is to keep the currently best h \"in a pocket\", while runing for i iterations(instead of until $E_{in} =0$). After the i iterations, the algorithm stops and returns the stored hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "In Linear Regression, we are in the __regression__ setting. We wish to find the hyperplane which best explain the observed datapoints in D. This hyperplane can then be used to output real numbers based on new input. The hypothesis set looks as follows:\n",
    "\n",
    "$$ H=\\{h(x)=w^T x \\mid w\\in \\mathbb{R}^{d+1}\\}$$\n",
    "\n",
    "Notice that it is similar to the hypothesis set of the Perceptron, the only difference is that here we don't take the sign() function on the signal: $w^T x$. \n",
    "\n",
    "#### Training\n",
    "In most learning situations we have to settle for a near-optimal solution. But in Linear Regression the in-sample error function, $E_{in}(h) =  \\frac{1}{N}\\sum_{i=1}^N (h(x_i)-y_i)^2$ called __least squares__,  is \"simple\" enough to allow us to find a __closed-form solution__. Thus we can - in a sort of one step learning process - pin down a weight vector $w^*$, which define the __optimal hyperplane__ for the given dataset. The equation looks as follows:\n",
    "\n",
    "$$w^\\ast = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "Here y is a column vector of all $y_i's$ of the training data, and X is the $n * (d+1)$ matrix where each row contains the entries of a single datapoint.\n",
    "\n",
    "#### Deriving $w^\\ast$\n",
    "\n",
    "Below is the steps for deriving the equation for $w^\\ast$. We start from the error function in the form above:\n",
    "\n",
    "$$E_{in}(h) = \\frac{1}{N} \\sum_{i=1}^N (h(x_i)-y_i)^2$$\n",
    " \n",
    "As any $ h\\in H $ is completely defined by some vector of weights w, we can replace h(x):\n",
    "    \n",
    "$$E_{in}(w) = \\frac{1}{N} \\sum_{i=1}^N (w^Tx_i-y_i)^2$$\n",
    "    \n",
    "This can be rewritten to matrix/vector form. Here $Xw -y =  [x_i^Tw-y_i,..,x_n^Tw-y_n]^T$.\n",
    "    \n",
    "$$E_{in}(w) = \\frac{1}{N} ||Xw-y||^2$$\n",
    "    \n",
    "We wish to minimize $E_{in}$  with respect to w, so we find the gradient consisting of all partial derivatives and set it equal to the 0-vector.\n",
    "\n",
    "For __a 1 variable, 2 datapoint, example__ $E_{in}(w)$ looks as follows when expanded ($x_{i,j}$ is the i'th feature of the j'th datapoint):\n",
    "\n",
    "$$ E_{in}(w) = \\frac{1}{N}\\big((x_{0,1} w_0 + x_{1,1} w_1 -y_1)^2 +(x_{0,2} w_0 + x_{1,2} w_1 -y_2)^2\\big)$$\n",
    "\n",
    "So when using the chain rule to compute the gradient for the expanded $E_{in}(w)$ above we get:\n",
    "\n",
    "\n",
    "\\begin{equation} \n",
    "       \\begin{split}\n",
    "        \\nabla E_{in}(w) &=  \\frac{1}{N}{\\begin{bmatrix}\n",
    "           \\frac{\\partial E_{in}}{\\partial w_0} \\\\\n",
    "           \\frac{\\partial E_{in}}{\\partial w_1} \\\\\n",
    "         \\end{bmatrix}}\\\\\\\\\n",
    "          &= \\frac{1}{N} \\begin{bmatrix}\n",
    "           2(x_{0,1} w_0 + x_{1,1} w_1 -y_1)\\cdot (x_{0,1} \\cdot 1 + 0 - 0) + 2(x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\cdot (x_{0,2} \\cdot 1 + 0 - 0)  \\\\\n",
    "           2(x_{0,1} w_0 + x_{1,1} w_1 -y_1)\\cdot (0 + x_{1,1} \\cdot 1 - 0) + 2(x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\cdot (0 + x_{1,1} \\cdot 1 - 0)  \\\\\n",
    "         \\end{bmatrix}\\\\\\\\\n",
    "          &= \\frac{2}{N} \\begin{bmatrix}\n",
    "           (x_{0,1} w_0 + x_{1,1} w_1 -y_1)\\cdot x_{0,1} + (x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\cdot x_{0,2}  \\\\\n",
    "           (x_{0,1} w_0 + x_{1,1} w_1 -y_1)\\cdot x_{1,1} + (x_{0,2} w_0 + x_{1,2} w_1 -y_2) \\cdot x_{1,1} \\\\\n",
    "         \\end{bmatrix}\\\\\\\\\n",
    "          &= \\frac{2}{N} X^T \\begin{bmatrix}\n",
    "          (x_{0,1} w_0 + x_{1,1} w_1 -y_1) + (x_{0,2} w_0 + x_{1,2} w_1 -y_2)   \\\\\n",
    "           (x_{0,1} w_0 + x_{1,1} w_1 -y_1) + (x_{0,2} w_0 + x_{1,2} w_1 -y_2)  \\\\\n",
    "         \\end{bmatrix}\\\\\\\\\n",
    "         &= \\frac{2}{N}X^T(Xw-y)\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "    \n",
    "\n",
    "    \n",
    "Now if we set it equal to the 0-vector, and rewrite abit we get(ignoring the normalization factor):\n",
    "    \n",
    "$$X^TXw =X^Ty$$\n",
    "    \n",
    "Now we can solve for w by multiplying with the inverse of $X^TX$:\n",
    "    \n",
    "$$w^\\ast =(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "And we have a formula for THE best w for the given data.\n",
    "\n",
    "$X^\\dagger = (X^TX)^{-1}X^T$ is called the pseudo inverse of X, because if you multiply X with it, you get the identity matrix.\n",
    "\n",
    "TODO skriv mere om inverse matricer\n",
    "\n",
    "#### Linear Regression for classification\n",
    "\n",
    "TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (classification)\n",
    "\n",
    "In Logstic Regression, the output is again a real number. But we can choose to interpret the output as a probability of belonging in either class 0 or 1, and thus if the output is greater than 0.5 we output \"class 1\", and we output \"class 0\" otherwise. This means we can use it for classification, with the nice biproduct of knowing the probability of each class. \n",
    "\n",
    "The hypothesis set looks a follows:\n",
    "\n",
    "$$H = \\{h(x) = \\sigma(w^Tx) | w \\in \\mathbb{R}^{d+1}\\}$$\n",
    "\n",
    "Here, $\\sigma()$ refers to the sigmoid squishification function:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$ \n",
    "\n",
    "Which compress the input into the range ]0,1[, such that $\\sigma(x)$ is close to 1 if x is a large positive number, and close to 0 if x is a large negative number.  \n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
